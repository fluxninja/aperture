// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: aperture/policy/language/v1/policy.proto

package com.fluxninja.generated.aperture.policy.language.v1;

public interface SchedulerOrBuilder extends
    // @@protoc_insertion_point(interface_extends:aperture.policy.language.v1.Scheduler)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * Output ports for the Scheduler component.
   * </pre>
   *
   * <code>.aperture.policy.language.v1.Scheduler.Outs out_ports = 1 [json_name = "outPorts"];</code>
   * @return Whether the outPorts field is set.
   */
  boolean hasOutPorts();
  /**
   * <pre>
   * Output ports for the Scheduler component.
   * </pre>
   *
   * <code>.aperture.policy.language.v1.Scheduler.Outs out_ports = 1 [json_name = "outPorts"];</code>
   * @return The outPorts.
   */
  com.fluxninja.generated.aperture.policy.language.v1.Scheduler.Outs getOutPorts();
  /**
   * <pre>
   * Output ports for the Scheduler component.
   * </pre>
   *
   * <code>.aperture.policy.language.v1.Scheduler.Outs out_ports = 1 [json_name = "outPorts"];</code>
   */
  com.fluxninja.generated.aperture.policy.language.v1.Scheduler.OutsOrBuilder getOutPortsOrBuilder();

  /**
   * <pre>
   * List of workloads to be used in scheduler.
   * Categorizing [flows](/concepts/flow-control/flow-control.md#flow) into workloads
   * allows for load-shedding to be "smarter" than just "randomly deny 50% of
   * requests". There are two aspects of this "smartness":
   * * Scheduler can more precisely calculate concurrency if it understands
   *   that flows belonging to different classes have different weights (eg.
   *   inserts vs lookups).
   * * Setting different priorities to different workloads lets the scheduler
   *   avoid dropping important traffic during overload.
   * Each workload in this list specifies also a matcher that's used to
   * determine which flow will be categorized into which workload.
   * In case of multiple matching workloads, the first matching one will be used.
   * If none of workloads match, `default_workload` will be used.
   * :::info
   * See also [workload definition in the concepts
   * section](/concepts/flow-control/concurrency-limiter.md#workload).
   * :::
   * </pre>
   *
   * <code>repeated .aperture.policy.language.v1.Scheduler.Workload workloads = 2 [json_name = "workloads"];</code>
   */
  java.util.List<com.fluxninja.generated.aperture.policy.language.v1.Scheduler.Workload> 
      getWorkloadsList();
  /**
   * <pre>
   * List of workloads to be used in scheduler.
   * Categorizing [flows](/concepts/flow-control/flow-control.md#flow) into workloads
   * allows for load-shedding to be "smarter" than just "randomly deny 50% of
   * requests". There are two aspects of this "smartness":
   * * Scheduler can more precisely calculate concurrency if it understands
   *   that flows belonging to different classes have different weights (eg.
   *   inserts vs lookups).
   * * Setting different priorities to different workloads lets the scheduler
   *   avoid dropping important traffic during overload.
   * Each workload in this list specifies also a matcher that's used to
   * determine which flow will be categorized into which workload.
   * In case of multiple matching workloads, the first matching one will be used.
   * If none of workloads match, `default_workload` will be used.
   * :::info
   * See also [workload definition in the concepts
   * section](/concepts/flow-control/concurrency-limiter.md#workload).
   * :::
   * </pre>
   *
   * <code>repeated .aperture.policy.language.v1.Scheduler.Workload workloads = 2 [json_name = "workloads"];</code>
   */
  com.fluxninja.generated.aperture.policy.language.v1.Scheduler.Workload getWorkloads(int index);
  /**
   * <pre>
   * List of workloads to be used in scheduler.
   * Categorizing [flows](/concepts/flow-control/flow-control.md#flow) into workloads
   * allows for load-shedding to be "smarter" than just "randomly deny 50% of
   * requests". There are two aspects of this "smartness":
   * * Scheduler can more precisely calculate concurrency if it understands
   *   that flows belonging to different classes have different weights (eg.
   *   inserts vs lookups).
   * * Setting different priorities to different workloads lets the scheduler
   *   avoid dropping important traffic during overload.
   * Each workload in this list specifies also a matcher that's used to
   * determine which flow will be categorized into which workload.
   * In case of multiple matching workloads, the first matching one will be used.
   * If none of workloads match, `default_workload` will be used.
   * :::info
   * See also [workload definition in the concepts
   * section](/concepts/flow-control/concurrency-limiter.md#workload).
   * :::
   * </pre>
   *
   * <code>repeated .aperture.policy.language.v1.Scheduler.Workload workloads = 2 [json_name = "workloads"];</code>
   */
  int getWorkloadsCount();
  /**
   * <pre>
   * List of workloads to be used in scheduler.
   * Categorizing [flows](/concepts/flow-control/flow-control.md#flow) into workloads
   * allows for load-shedding to be "smarter" than just "randomly deny 50% of
   * requests". There are two aspects of this "smartness":
   * * Scheduler can more precisely calculate concurrency if it understands
   *   that flows belonging to different classes have different weights (eg.
   *   inserts vs lookups).
   * * Setting different priorities to different workloads lets the scheduler
   *   avoid dropping important traffic during overload.
   * Each workload in this list specifies also a matcher that's used to
   * determine which flow will be categorized into which workload.
   * In case of multiple matching workloads, the first matching one will be used.
   * If none of workloads match, `default_workload` will be used.
   * :::info
   * See also [workload definition in the concepts
   * section](/concepts/flow-control/concurrency-limiter.md#workload).
   * :::
   * </pre>
   *
   * <code>repeated .aperture.policy.language.v1.Scheduler.Workload workloads = 2 [json_name = "workloads"];</code>
   */
  java.util.List<? extends com.fluxninja.generated.aperture.policy.language.v1.Scheduler.WorkloadOrBuilder> 
      getWorkloadsOrBuilderList();
  /**
   * <pre>
   * List of workloads to be used in scheduler.
   * Categorizing [flows](/concepts/flow-control/flow-control.md#flow) into workloads
   * allows for load-shedding to be "smarter" than just "randomly deny 50% of
   * requests". There are two aspects of this "smartness":
   * * Scheduler can more precisely calculate concurrency if it understands
   *   that flows belonging to different classes have different weights (eg.
   *   inserts vs lookups).
   * * Setting different priorities to different workloads lets the scheduler
   *   avoid dropping important traffic during overload.
   * Each workload in this list specifies also a matcher that's used to
   * determine which flow will be categorized into which workload.
   * In case of multiple matching workloads, the first matching one will be used.
   * If none of workloads match, `default_workload` will be used.
   * :::info
   * See also [workload definition in the concepts
   * section](/concepts/flow-control/concurrency-limiter.md#workload).
   * :::
   * </pre>
   *
   * <code>repeated .aperture.policy.language.v1.Scheduler.Workload workloads = 2 [json_name = "workloads"];</code>
   */
  com.fluxninja.generated.aperture.policy.language.v1.Scheduler.WorkloadOrBuilder getWorkloadsOrBuilder(
      int index);

  /**
   * <pre>
   * WorkloadParameters to be used if none of workloads specified in `workloads` match.
   * </pre>
   *
   * <code>.aperture.policy.language.v1.Scheduler.WorkloadParameters default_workload_parameters = 3 [json_name = "defaultWorkloadParameters", (.grpc.gateway.protoc_gen_openapiv2.options.openapiv2_field) = { ... }</code>
   * @return Whether the defaultWorkloadParameters field is set.
   */
  boolean hasDefaultWorkloadParameters();
  /**
   * <pre>
   * WorkloadParameters to be used if none of workloads specified in `workloads` match.
   * </pre>
   *
   * <code>.aperture.policy.language.v1.Scheduler.WorkloadParameters default_workload_parameters = 3 [json_name = "defaultWorkloadParameters", (.grpc.gateway.protoc_gen_openapiv2.options.openapiv2_field) = { ... }</code>
   * @return The defaultWorkloadParameters.
   */
  com.fluxninja.generated.aperture.policy.language.v1.Scheduler.WorkloadParameters getDefaultWorkloadParameters();
  /**
   * <pre>
   * WorkloadParameters to be used if none of workloads specified in `workloads` match.
   * </pre>
   *
   * <code>.aperture.policy.language.v1.Scheduler.WorkloadParameters default_workload_parameters = 3 [json_name = "defaultWorkloadParameters", (.grpc.gateway.protoc_gen_openapiv2.options.openapiv2_field) = { ... }</code>
   */
  com.fluxninja.generated.aperture.policy.language.v1.Scheduler.WorkloadParametersOrBuilder getDefaultWorkloadParametersOrBuilder();

  /**
   * <pre>
   * Automatically estimate the size of a request in each workload, based on
   * historical latency. Each workload's `tokens` will be set to average
   * latency of flows in that workload during last few seconds (exact duration
   * of this average can change).
   * </pre>
   *
   * <code>bool auto_tokens = 4 [json_name = "autoTokens", (.grpc.gateway.protoc_gen_openapiv2.options.openapiv2_field) = { ... }</code>
   * @return The autoTokens.
   */
  boolean getAutoTokens();

  /**
   * <pre>
   * Timeout as a factor of tokens for a flow in a workload
   * If a flow is not able to get tokens within `timeout_factor` * `tokens` of duration,
   * it will be rejected.
   * This value impacts the prioritization and fairness because the larger the timeout the higher the chance a request has to get scheduled.
   * </pre>
   *
   * <code>double timeout_factor = 5 [json_name = "timeoutFactor", (.grpc.gateway.protoc_gen_openapiv2.options.openapiv2_field) = { ... }</code>
   * @return The timeoutFactor.
   */
  double getTimeoutFactor();

  /**
   * <pre>
   * Max Timeout is the value with which the flow timeout calculated by `timeout_factor` is capped
   * :::caution
   * This timeout needs to be strictly less than the timeout set on the
   * client for the whole GRPC call:
   * * in case of envoy, timeout set on `grpc_service` used in `ext_authz` filter,
   * * in case of libraries, timeout configured... TODO.
   * We're using fail-open logic in integrations, so if the GRPC timeout
   * fires first, the flow will end up being unconditionally allowed while
   * it're still waiting on the scheduler.
   * To avoid such cases, the end-to-end GRPC timeout should also contain
   * some headroom for constant overhead like serialization, etc. Default
   * value for GRPC timeouts is 500ms, giving 50ms of headeroom, so when
   * tweaking this timeout, make sure to adjust the GRPC timeout accordingly.
   * :::
   * </pre>
   *
   * <code>.google.protobuf.Duration max_timeout = 6 [json_name = "maxTimeout", (.grpc.gateway.protoc_gen_openapiv2.options.openapiv2_field) = { ... }</code>
   * @return Whether the maxTimeout field is set.
   */
  boolean hasMaxTimeout();
  /**
   * <pre>
   * Max Timeout is the value with which the flow timeout calculated by `timeout_factor` is capped
   * :::caution
   * This timeout needs to be strictly less than the timeout set on the
   * client for the whole GRPC call:
   * * in case of envoy, timeout set on `grpc_service` used in `ext_authz` filter,
   * * in case of libraries, timeout configured... TODO.
   * We're using fail-open logic in integrations, so if the GRPC timeout
   * fires first, the flow will end up being unconditionally allowed while
   * it're still waiting on the scheduler.
   * To avoid such cases, the end-to-end GRPC timeout should also contain
   * some headroom for constant overhead like serialization, etc. Default
   * value for GRPC timeouts is 500ms, giving 50ms of headeroom, so when
   * tweaking this timeout, make sure to adjust the GRPC timeout accordingly.
   * :::
   * </pre>
   *
   * <code>.google.protobuf.Duration max_timeout = 6 [json_name = "maxTimeout", (.grpc.gateway.protoc_gen_openapiv2.options.openapiv2_field) = { ... }</code>
   * @return The maxTimeout.
   */
  com.google.protobuf.Duration getMaxTimeout();
  /**
   * <pre>
   * Max Timeout is the value with which the flow timeout calculated by `timeout_factor` is capped
   * :::caution
   * This timeout needs to be strictly less than the timeout set on the
   * client for the whole GRPC call:
   * * in case of envoy, timeout set on `grpc_service` used in `ext_authz` filter,
   * * in case of libraries, timeout configured... TODO.
   * We're using fail-open logic in integrations, so if the GRPC timeout
   * fires first, the flow will end up being unconditionally allowed while
   * it're still waiting on the scheduler.
   * To avoid such cases, the end-to-end GRPC timeout should also contain
   * some headroom for constant overhead like serialization, etc. Default
   * value for GRPC timeouts is 500ms, giving 50ms of headeroom, so when
   * tweaking this timeout, make sure to adjust the GRPC timeout accordingly.
   * :::
   * </pre>
   *
   * <code>.google.protobuf.Duration max_timeout = 6 [json_name = "maxTimeout", (.grpc.gateway.protoc_gen_openapiv2.options.openapiv2_field) = { ... }</code>
   */
  com.google.protobuf.DurationOrBuilder getMaxTimeoutOrBuilder();
}
