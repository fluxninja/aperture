# vim: ft=bzl sw=4
v1alpha1.extension_repo(name='default', url='https://github.com/tilt-dev/tilt-extensions', ref='HEAD')
load("ext://namespace", "namespace_create", "namespace_inject")
load('ext://uibutton', 'cmd_button', 'location')

#################
### CONSTANTS ###
#################
IMG_REPO = "docker.io/fluxninja"
GIT_ROOT = str(local("git rev-parse --show-toplevel")).strip()
PLAYGROUND_ROOT = GIT_ROOT + "/playground"
TANKA_ROOT = GIT_ROOT + "/playground/tanka"
TILT_OBJECTS = []
APERTURE_GO_REPO_PATH = GIT_ROOT + "/sdks/aperture-go"
APERTURE_JAVA_REPO_PATH = GIT_ROOT + "/sdks/aperture-java"
APERTURE_JS_REPO_PATH = GIT_ROOT + "/sdks/aperture-js"
APERTURE_AGENT_NS = "aperture-agent"
APERTURE_CONTROLLER_NS = "aperture-controller"
APERTURE_GO_EXAMPLE_NS = "aperture-go-example"
APERTURE_JS_EXAMPLE_NS = "aperture-js-example"
APERTURE_JAVA_EXAMPLE_NS = "aperture-java-example"
APERTURE_DEMOAPP_NS = "demoapp"
APERTURE_ISTIO_NS = "istio-system"
TANKA = "tk"
SKIP_LOCAL_CMDS = False  # If set to true, skips actually running local commands (useful for developing and testing Tiltfile itself)
# Mutable global state, since global values are not mutable
GLOBAL_STATE = {}
GLOBAL_RAN_TANKA_DEPS_KEY = "RAN_TANKA_DEPS"

# Dependency tree and service configuration.
# Top level maps NAMESPACE to services
# Services are a mapping of service name to their config, which consists of:
# "labels": A list of labels to apply to the service
# "needs": list of services (from any namespace) which need to be brought up for the service to work
# "tkenv": Path to
# "images": list of dicts, defining arguments passed to build_docker_img. Arguments are:
## "tag": Docker image tag. If specified, image is pulled, else, it is built
## "ref": Docker image name
## "live_update": Live update callback
## "context": Context to build the image in
## "dockerfile": Location of Dockerfile to build the image from
## "ssh": Unused. TODO: Remove
# "port_forwards": any value accepted by k8s_resource's port_forwards argument. In addition to that, it can be a dict describing multiple port_forwards (mapping name to port_forwards, as above)
# "child_resources"
# "pvc_selectors"
# "kinds"
DEP_TREE = {
    APERTURE_CONTROLLER_NS: {
        "aperture-grafana": {
            "labels": ["ApertureController"],
            # construct directory path to tkenv
            "tkenv": TANKA_ROOT + "/apps/aperture-grafana",
            "child_resources": [
                {
                    "new_name": "grafana-operator-bootstrap",
                    "extra_objects": [
                        ("integreatly.org$", "CustomResourceDefinition", None),
                    ]
                },
                {
                    "workload": "aperture-grafana-operator",
                    "new_name": "grafana-operator",
                    "resource_deps": ["grafana-operator-bootstrap"],
                    "extra_objects": [
                        ("^aperture-grafana-operator", None, APERTURE_CONTROLLER_NS)
                    ],
                    "ignored_objects": [
                        ("^aperture-grafana-operator", "Deployment", APERTURE_CONTROLLER_NS)
                    ]
                },
                {
                    "new_name": "grafana",
                    "resource_deps": ["grafana-operator"],
                    "extra_objects": [
                        "aperture-grafana:grafana",
                        "controller-prometheus:grafanadatasource"
                    ]
                },
                {
                    "new_name": "dashboards",
                    "resource_deps": ["grafana-operator-bootstrap"],
                    "extra_objects": [
                        (None, "GrafanaDashboard", APERTURE_CONTROLLER_NS)
                    ]
                },
                {
                    "new_name": "grafana-data-sources",
                    "resource_deps": ["grafana-operator-bootstrap"],
                    "extra_objects": [
                        (None, "GrafanaDataSource", APERTURE_CONTROLLER_NS)
                    ]
                }
            ]
        },
        "controller": {
            "labels": ["ApertureController"],
            "tkenv": TANKA_ROOT + "/apps/aperture-controller",
            "images": [
                {
                    "ref": "aperture-controller",
                    "context": GIT_ROOT,
                    "dockerfile": GIT_ROOT + "/cmd/aperture-controller/Dockerfile",
                    "ssh": "default",
                    "build_args":{'RACE': ""},
                },
                {
                    "ref": "aperture-operator",
                    "context": GIT_ROOT,
                    "dockerfile": GIT_ROOT + "/operator/Dockerfile",
                    "ssh": "default",
                    "ignore": ["cmd"],
                }
            ],
            "port_forwards": {
                "controller-prometheus-server": "9090:9090",
                "controller-etcd": "2379:2379",
            },
            "pvc_selectors": ["app.kubernetes.io/name=etcd"],
            "kinds": [
                dict(kind="Controller", image_object={'json_path': '{.spec.image}', 'repo_field': 'repository', 'tag_field': 'tag'}, pod_readiness='wait'),
            ],
            "child_resources": [
                {
                    "new_name": "controller-operator-bootstrap",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        "controllers.fluxninja.com:customresourcedefinition",
                        "policies.fluxninja.com:customresourcedefinition",
                        "aperture-controller-defaulter:mutatingwebhookconfiguration",
                    ]
                },
                {
                    "workload": "controller",
                    "new_name": "controller",
                    "resource_deps": ["controller-etcd", "controller-aperture-controller-manager", "controller-prometheus"],
                    "extra_pod_selectors": [
                        {'app.kubernetes.io/component':  'controller', 'app.kubernetes.io/managed-by': 'aperture-operator'},
                    ]
                },
                {
                    "workload": "controller-aperture-controller-manager",
                    "resource_deps": ["controller-operator-bootstrap"],
                    "extra_objects": [
                        ("^controller-aperture-controller", "Role|RoleBinding|ServiceAccount", APERTURE_CONTROLLER_NS),
                        ("^controller-aperture-controller", "ClusterRole|ClusterRoleBinding"),
                    ],
                    "ignored_objects": [
                        ("^controller-aperture-controller-manager", "Service|Deployment", APERTURE_CONTROLLER_NS),
                    ]
                },
                {
                    "workload": "controller-etcd",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        ("^controller-etcd", None),
                    ],
                    "ignored_objects": [
                        ("^controller-etcd", "Service|StatefulSet")
                    ]
                },
                {
                    "workload": "controller-prometheus-server",
                    "new_name": "controller-prometheus",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        ("^controller-prometheus", None),
                    ],
                    "ignored_objects": [
                        ("^controller-prometheus", "Service|Deployment"),
                        "controller-prometheus:grafanadatasource"
                    ]
                },
                {
                    "workload": "controller-kube-state-metrics",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        ("^controller-kube-state-metrics", None),
                    ],
                    "ignored_objects": [
                        ("^controller-kube-state-metrics", "Service|Deployment", APERTURE_CONTROLLER_NS),
                    ]
                },
                {
                    "new_name": "policies",
                    "resource_deps": ["cluster-bootstrap", "controller-service"],
                    "extra_objects": [
                        ("service1-demo-app$", "policy", APERTURE_CONTROLLER_NS),
                    ]
                },
            ]
        }
    },
    APERTURE_AGENT_NS: {
        "agent": {
            "labels": ["ApertureAgent"],
            "needs": ["controller"],
            "tkenv": TANKA_ROOT + "/apps/aperture-agent",
            "images": [
                {
                    "ref": "aperture-agent",
                    "context": GIT_ROOT,
                    "dockerfile": GIT_ROOT + "/cmd/aperture-agent/Dockerfile",
                    "ssh": "default",
                    "build_args":{'RACE':""},
                }
            ],
            "kinds": [
                dict(kind="Agent", image_object={'json_path': '{.spec.image}', 'repo_field': 'repository', 'tag_field': 'tag'}, pod_readiness='wait'),
            ],
            "child_resources": [
                {
                    "new_name": "agent-operator-bootstrap",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        "agents.fluxninja.com:customresourcedefinition",
                         "aperture-agent-defaulter:mutatingwebhookconfiguration",
                    ]
                },
                {
                    "workload": "agent",
                    "new_name": "agent",
                    "resource_deps": ["controller-etcd", "agent-aperture-agent-manager", "controller-prometheus"],
                    "extra_pod_selectors": [
                        {'app.kubernetes.io/component':  'agent', 'app.kubernetes.io/managed-by': 'aperture-operator'},
                    ]
                },
                {
                    "workload": "agent-aperture-agent-manager",
                    "resource_deps": ["agent-operator-bootstrap"],
                    "extra_objects": [
                        ("^agent-aperture-agent", "Role|RoleBinding|ServiceAccount", APERTURE_AGENT_NS),
                        ("^agent-aperture-agent", "ClusterRole|ClusterRoleBinding"),
                    ],
                    "ignored_objects": [
                        ("^agent-aperture-agent-manager", "Service|Deployment", APERTURE_AGENT_NS),
                    ]
                }
            ]
        }
    },
    APERTURE_ISTIO_NS: {
        "istio": {
            "labels": ["Istio"],
            "needs": ["controller", "agent"],
            "tkenv": TANKA_ROOT + "/apps/istio",
            "child_resources": [
                {
                    "new_name": "istio-bootstrap",
                    "extra_objects": [
                        "istio:configmap",
                        "aperture-envoy-filter:envoyfilter",
                        "istiod-default-validator:validatingwebhookconfiguration",
                        ("istio.io$", "customresourcedefinition"),
                        (None, "EnvoyFilter", "istio-system"),
                        ("^istio-reader", None),
                        ("^istio-sidecar", None),
                    ]
                },
                {
                    "workload": "istiod",
                    "resource_deps": ["istio-bootstrap"],
                    "extra_objects": [
                        ("^istiod", None),
                    ],
                    "ignored_objects": [
                        ("^istiod", "Service|Deployment")
                    ]
                },
                {
                    "workload": "gateway",
                    "resource_deps": ["istio-bootstrap", "istiod"],
                    "extra_objects": [
                        ("^gateway", None),
                    ],
                    "ignored_objects": [
                        ("^gateway", "Service|Deployment"),
                    ]
                },
            ]
        },
    },
    APERTURE_DEMOAPP_NS: {
        "graphql-demo-app": {
            "labels": ["GraphQLDemoApplications"],
            "tkenv": TANKA_ROOT + "/apps/graphql-demoapp",
            "needs": ["istio"],
            "images": [
                {
                    "ref": "graphql-demo-app",
                    "context": GIT_ROOT + "/playground/graphql_demo_app",
                    "ssh": "default",
                }
            ],
            "child_resources": [
                {
                    "workload": "service-graphql-demo-app",
                    "resource_deps": ["cluster-bootstrap", "istiod"],
                    "extra_objects": [
                        "service-graphql-demo-app:serviceaccount"
                    ]
                },
                {
                    "workload": "wavepool-graphql-generator",
                    "resource_deps": ["service-graphql-demo-app", "policies"],
                    "extra_objects": [
                        "wavepool-graphql-config:configmap"
                    ],
                    "extra_pod_selectors": [
                        {'app.kubernetes.io/component':  'wavepool-graphql-generator'},
                    ]
                },
            ]
        },
        "demo-app": {
            "labels": ["DemoApplications"],
            "tkenv": TANKA_ROOT + "/apps/demoapp",
            "needs": ["istio"],
            "images": [
                {
                    "ref": "demo-app",
                    "context": GIT_ROOT + "/playground/demo_app",
                    "ssh": "default",
                }
            ],
            "child_resources": [
                {
                    "workload": "service1-demo-app",
                    "resource_deps": ["cluster-bootstrap", "istiod"],
                    "extra_objects": [
                        "service1-demo-app:serviceaccount"
                    ]
                },
                {
                    "workload": "service2-demo-app",
                    "resource_deps": ["cluster-bootstrap", "istiod"],
                    "extra_objects": [
                        "service2-demo-app:serviceaccount"
                    ]
                },
                {
                    "workload": "service3-demo-app",
                    "resource_deps": ["cluster-bootstrap", "istiod"],
                    "extra_objects": [
                        "service3-demo-app:serviceaccount"
                    ]
                },
                {
                    "workload": "wavepool-generator",
                    "resource_deps": ["service1-demo-app", "policies"],
                    "extra_objects": [
                        "wavepool-config:configmap"
                    ],
                    "extra_pod_selectors": [
                        {'app.kubernetes.io/component':  'wavepool-generator'},
                    ]
                },
            ]
        },
    },
    APERTURE_GO_EXAMPLE_NS: {
        "aperture-go-example": {
            "labels": ["Aperture"],
            "needs": ["agent"],
            "tkenv": TANKA_ROOT + "/apps/aperture-go-example",
            "images": [
                {
                    "ref": "aperture-go-example",
                    "context": APERTURE_GO_REPO_PATH,
                }
            ],
            "child_resources": [
                {
                    "workload": "aperture-go-example",
                    "resource_deps": ["agent", "controller", "cluster-bootstrap"],
                }
            ]
        },
    },
    APERTURE_JS_EXAMPLE_NS: {
        "aperture-js-example": {
            "labels": ["Aperture"],
            "needs": ["agent"],
            "tkenv": TANKA_ROOT + "/apps/aperture-js-example",
            "images": [
                {
                    "ref": "aperture-js-example",
                    "context": APERTURE_JS_REPO_PATH if os.path.exists(APERTURE_JS_REPO_PATH) else "https://github.com/fluxninja/aperture-js.git#main",
                }
            ],
            "child_resources": [
                {
                    "workload": "aperture-js-example",
                    "resource_deps": ["agent", "controller", "cluster-bootstrap"],
                }
            ]
        },
    },
    APERTURE_JAVA_EXAMPLE_NS: {
        "aperture-java-example": {
            "labels": ["Aperture"],
            "needs": ["agent"],
            "tkenv": TANKA_ROOT + "/apps/aperture-java-example",
            "images": [
                {
                    "ref": "aperture-java-example",
                    "context": APERTURE_JAVA_REPO_PATH,
                }
            ],
            "child_resources": [
                {
                    "workload": "aperture-java-example",
                    "resource_deps": ["agent", "controller", "cluster-bootstrap"],
                }
            ]
        },
    },
}

NS_CONFIG = {
    APERTURE_DEMOAPP_NS: {
        "labels": ["istio-injection: enabled"],
    },
}

###############
### HELPERS ###
###############

def verbose(*msg):
    if VERBOSE:
        print("VERBOSE:", *msg)

def trace(*msg):
    if TRACE:
        print("TRACE:", *msg)

def merge_dicts(*dicts):
    merged = {}
    for d in dicts:
        merged.update(d)
    return merged

def build_remote_docker_img(ref, context, **kwargs):
    if kwargs:
        fail("build_remote_docker_img doesn't support kwargs: {}".format(kwargs))
    custom_build(
        ref=ref,
        deps=[],
        # Would be better to run as list[str], but then we can't reference the env var
        command='docker build -t "$EXPECTED_REF" "{context}"'.format(context=context),
    )

def build_docker_img(ref, context, dockerfile="Dockerfile", **kwargs):
    # Wrapper around docker_build to not repeat image repo,
    print(
        "Will build",
        ref,
        "with context of",
        context,
        "using",
        dockerfile,
        "and",
        kwargs,
    )
    ref = IMG_REPO.rstrip("/") + "/" + ref
    if context.startswith("https://"):
        build_remote_docker_img(ref, context, **kwargs)
    else:
        if not context.startswith("/"):
            context = GIT_ROOT + "/" + context
        if not dockerfile.startswith("/"):
            dockerfile = context + "/" + (dockerfile or "Dockerfile")
        docker_build(ref, context, dockerfile=dockerfile, **kwargs)


def pull_docker_img(ref, tag):
    print("Will pull", ref, "with version", tag)
    ref = IMG_REPO.rstrip("/") + "/" + ref
    custom_build(
      ref=ref,
      command=["docker", "pull", ref + ":" + tag],
      deps=[],
      tag=tag,
    )


def _convert_yaml_into_objects(yaml):
    if type(yaml) == "string":
        return read_yaml_stream(yaml)
    elif type(yaml) == "blob":
        return decode_yaml_stream(yaml)
    else:
        fail('only takes string or blob, got: %s' % type(yaml))


def update_tilt_objects(objects):
    for obj in objects:
        object_name = obj["metadata"]["name"].lower()
        object_name = object_name.replace(":", "\\:")

        object_kind = obj["kind"].lower()
        object_namespace = obj["metadata"].get("namespace")

        object_selector = ":".join([object_name, object_kind])
        if object_namespace:
            object_selector += ":%s" % object_namespace

        TILT_OBJECTS.append(object_selector)


def wrapped_k8s_yaml(yaml, allow_duplicates=False):
    objects = _convert_yaml_into_objects(yaml)
    update_tilt_objects(objects)

    k8s_yaml(yaml, allow_duplicates)


def refresh_chart_dependencies(chart_dir):
    dep_output = local(command=["helm", "dependency", "list"], dir=chart_dir)
    lines = str(dep_output).splitlines()
    for number in range(1, len(lines) - 1):
        if not lines[number].strip().endswith("ok"):
            local(command=["helm", "dependency", "update"], dir=chart_dir)
            return


def render_tanka(environment, env_vars):
    print(
        "Rendering tanka environment",
        environment,
    )
    base_dir = environment.split("apps/")[0] or "./"

    if SKIP_LOCAL_CMDS:
        return

    if not GLOBAL_STATE.get(GLOBAL_RAN_TANKA_DEPS_KEY):
        # Get base directory for Jsonnet Bundler execution
        # tanka/apps/default -> tanka/
        # apps/default -> ./
        # Make sure jb dependencies are up to date
        local(command=["./scripts/jb_install.sh"])

        # Refreshing the dependencies for the Agent chart
        refresh_chart_dependencies(base_dir + "charts/aperture-agent")
        refresh_chart_dependencies(base_dir + "charts/aperture-controller")

        # Fetch any vendored charts that are not included in our repository
        local(command=[TANKA, "tool", "charts", "vendor"], dir=base_dir)

        # Skip refreshing deps the second time
        GLOBAL_STATE[GLOBAL_RAN_TANKA_DEPS_KEY] = True

    trace("Rendering tanka with env: %s" % env_vars)
    # Generate manifests with tanka
    wrapped_k8s_yaml(
        local(
            quiet=True,
            command=[
                TANKA,
                "--ext-str=projectRoot=" + base_dir,
                "show",
                # Allow dumping the templates to stdout,
                # instead of using tk export to generate files
                # and then have to manage deleting them etc.
                "--dangerous-allow-redirect",
                environment
            ],
            env=env_vars
        )
    )


def print_available_resources(dep_tree):
    print("Following resources are available:")
    for ns, resources in dep_tree.items():
        print(ns)
        for resource in resources.keys():
            print("   ", resource)


def schedule_pvc_deletion(namespace, selector):
    # Cleanup DB
    # Marks PVC for deletion, but doesn't wait since finalizers need to run first
    # and they will be ran later when tilt deletes created resources
    local(
        command=[
            "kubectl",
            "--namespace",
            namespace,
            "delete",
            "pvc",
            "--selector",
            selector,
            "--wait=false",
        ]
    )

#####################
### OPTION PARSER ###
#####################


def invert_dep_tree(dep_tree):
    """Returns dict of resources, with injected namespace into resource dict"""
    inverted = {}
    for ns, resources in dep_tree.items():
        for resource, resource_info in resources.items():
            info_copy = dict(resource_info)
            info_copy["namespace"] = ns
            inverted[resource] = info_copy
    return inverted


def get_resource_list_to_deploy(names, run_only_listed, dep_tree, inv_dep_tree):
    """
    Recursively convert list of NS and service names to set of required services
    If a given name is both a name of resource and namespace, it is assumed to be resource
    """
    if "all" in names:
        names = inv_dep_tree.keys()
    resources = (
        []
    )  # It'd be faster and better to use set here, but Starlark set only has .union method
    for name in names:
        if name in inv_dep_tree:  # It's resource name
            resources.append(name)
            if not run_only_listed:
                # We could recursively resolve dependencies, but then we risk getting in a dependency loop
                resource_info = inv_dep_tree[name]
                dependencies = resource_info.get("needs", [])
                while len(dependencies) > 0:
                    dep = dependencies.pop()
                    if dep not in resources:
                        resources.append(dep)
                        dependencies.extend(inv_dep_tree[dep].get("needs", []))
        elif name in dep_tree:  # It's a namespace
            resource_names = dep_tree[name].keys()
            resources.extend(
                get_resource_list_to_deploy(resource_names, run_only_listed, dep_tree, inv_dep_tree)
            )
        else:
            fail("Unable to find " + name + " as neither resource nor namespace name")
    return set(resources)


def parse_kubernetes_object_selector(selector):
    """Parse string representing kubernetes object selector into its parts

    Tilt defines kubernetes object selectors as name[:kind[:namespace]], and kubernetes
    names can contain colons. This function parses given selector into its parts, while
    making sure that name with colons is properly handled.
    """
    parts = selector.split(":")

    name, kind, namespace = "", None, None
    # First, get name from parts - names in kubernetes may contain : so we must
    # rebuild it here, probably from multiple parts.
    while True:
        name += parts.pop(0).lower()
        if not name.endswith("\\"):
            break
        name = name[:-1] + ":"
        if not parts:
            fail("Unable to parse selector '%s'" % selector)

    if parts:
        kind = parts.pop(0).lower()

    if parts:
        namespace = parts.pop(0).lower()

    if parts:
        fail("Unable to parse selector '%s'" % selector)

    return (name, kind, namespace)


MATCHER_MATCH_START = 1
MATCHER_MATCH_END = 2
MATCHER_MATCH_FULL = 3
MATCHER_MATCH_EMPTY = 4
def match_kubernetes_object_selector_with_glob(selector_glob):
    """Returns all kubernetes objects known by tilt that match given matcher and string

    Starlark has no regex engine, so to get all matching objects we must cheat a bit. This
    function requires matcher and kind arguments, both strings: kind is lowercase kubernetes
    kind, and matcher is a string starting with ^ or ending with $, converting them into
    startswith() and endwith() respectively.
    """
    matcher_name = selector_glob[0]
    matcher_kind = selector_glob[1]
    if len(selector_glob) == 3:
        matcher_namespace = selector_glob[2]
    else:
        matcher_namespace = None

    if not matcher_name:
        matcher_type = MATCHER_MATCH_EMPTY
    elif matcher_name[0] == "^" and matcher_name[-1] == "$":
        matcher_name = matcher_name[1:-1]
        matcher_type = MATCHER_MATCH_FULL
    elif matcher_name[0] == "^":
        matcher_name = matcher_name[1:]
        matcher_type = MATCHER_MATCH_START
    elif matcher_name[-1] == "$":
        matcher_name = matcher_name[:-1]
        matcher_type = MATCHER_MATCH_END
    else:
        fail("Need to specify starting or ending matcher token for '%s'" % (matcher_name))
    trace("Running glob object selector with name=%s, kind=%s and namespace=%s" % (matcher_name, matcher_kind, matcher_namespace))

    matched_objs = []
    for obj in TILT_OBJECTS:
        name, kind, namespace = parse_kubernetes_object_selector(obj)

        if matcher_namespace and matcher_namespace != namespace:
            continue

        if matcher_kind:
            kinds = [k.lower() for k in matcher_kind.split("|")]
            if kind not in kinds:
                continue

        if matcher_type == MATCHER_MATCH_START and name.startswith(matcher_name):
            matched_objs.append(obj)
        elif matcher_type == MATCHER_MATCH_END and name.endswith(matcher_name):
            matched_objs.append(obj)
        elif matcher_type == MATCHER_MATCH_FULL and name == matcher_name:
            matched_objs.append(obj)
        elif matcher_type == MATCHER_MATCH_EMPTY:
            matched_objs.append(obj)

    return matched_objs


def match_kubernetes_object_selector(selector):
    """Find tilt object matching given kubernetes object selector

    Given a valid selector, find a matching object in a list of objects known
    to tilt. Fail if more than one object matches the given selector.
    """
    matched_obj = None
    parsed_selector = parse_kubernetes_object_selector(selector)

    for tilt_obj in TILT_OBJECTS:
        parsed_object = parse_kubernetes_object_selector(tilt_obj)
        parsed_object_without_ns = (parsed_object[0], parsed_object[1], None)
        # If selector matches given object (check both with and without namespace as selectors may be partial),
        # keep matched object reference and continue iterating the list to see if selector matches multiple
        # objects, which is an error.
        if parsed_selector == parsed_object or parsed_selector == parsed_object_without_ns:
            if matched_obj:
                fail("Multiple tilt objects matched selector '%s'" % (selector))
            matched_obj = tilt_obj

    return matched_obj


def _resolve_objects_match_objects(matchers, fail_on_missing=False):
    """Helper used for creating a list of all tilt objects matching matchers"""
    matched_objs = []
    for matcher in matchers:
        if type(matcher) == "string":
            matched = match_kubernetes_object_selector(matcher)
            if not matched and fail_on_missing:
                fail("Found no match for selector '%s'" % matcher)
            elif matched and matched not in matched_objs:
                matched_objs += [matched]
        else:
            matched = match_kubernetes_object_selector_with_glob(matcher)
            matched_not_present = [match for match in matched if match not in matched_objs]
            matched_objs.extend(matched_not_present)
    return matched_objs


def resolve_objects(included_matchers, ignored_matchers):
    """Resolve all objects from included_matchers into tilt objects

    For the included_matches list, get all objects from TILT_OBJECTS that are
    matching items of this list, and filter out objects that match items of the
    ignored_matchers list. This is useful when you want to use wide includes, and
    then exclude some subset of the matched items.
    """
    matched_objs = _resolve_objects_match_objects(included_matchers, fail_on_missing=True)
    verbose("Include matchers found", matched_objs)
    ignored_objs = _resolve_objects_match_objects(ignored_matchers)
    verbose("Ignore matchers found", ignored_objs)

    for obj in ignored_objs:
        if obj in matched_objs:
            matched_objs.remove(obj)

    for obj in matched_objs:
        TILT_OBJECTS.remove(obj)

    return matched_objs


#################################
### MAIN RESOURCE DECLARATION ###
#################################


def forward_port_delayed(workload, labels, resource_deps, namespace, service, local_port, remote_port, extra_env={}, readiness_probe=None):
    if not readiness_probe:
        readiness_probe=probe(exec=exec_action(['lsof', '-i', ":%s" % local_port])) # Treat as success if some process listens on this port
    local_resource(
        name=workload,
        labels=labels,
        resource_deps=resource_deps,
        serve_cmd=['./scripts/portforward.sh'],
        serve_env=merge_dicts(
            {
                "NAMESPACE": namespace,
                "RESOURCE_NAME": service,
                "LOCAL_PORT": str(local_port),
                "REMOTE_PORT": str(remote_port),
            },
            extra_env,
        ),
        readiness_probe=readiness_probe,
    )


def declare_resources(resources, dep_tree, inv_dep_tree,race_arg):
    """
    Declares resource-specific actions (images, ports, etc)
    Generating manifests with tanka is done separately, in order to speed up builds
    """
    for resource in resources:
        resource_info = inv_dep_tree[resource]
        labels = resource_info.get("labels", [])
        if race_arg:
            images = resource_info.get("images",{})
            if resource == "controller":
                images[0]["build_args"]["RACE"] = "Yes"
            if resource == "agent":
                images[0]["build_args"]["RACE"] = "Yes"
        for image in resource_info.get("images", []):
            if 'tag' in image.keys():
                # We will be pulling existing image
                if not set(image.keys()) == set(['tag', 'ref']):
                    fail("Invalid image configuration: {img}".format(img=image))
                pull_docker_img(**image)
            else:
                # We build images depending on what we want to deploy, and so live_update
                # steps have to be a lambda, so that they are only created when we realize
                # the image.
                live_update_callable = image.get("live_update")
                if live_update_callable:
                    image["live_update"] = live_update_callable()
                build_docker_img(**image)

        pf = resource_info.get("port_forwards", None)

        if pf:
            if type(pf) == "dict":
                # We have multiple pf resources to define
                for pf_name, pf_value in pf.items():
                    k8s_resource(pf_name, labels=labels, port_forwards=pf_value)
            else:
                k8s_resource(resource, labels=labels, port_forwards=pf)

    # Create mapping of namespace to list of resources within the namespace
    ns_resource_list = {}
    for resource in resources:
        resource_info = inv_dep_tree[resource]
        ns = resource_info["namespace"]
        # setdefault is a bit misleading:
        # it gets value for given key, if it exists
        # if it doesn't, it puts the default value under that key
        # and returns the selected value
        ns_resource_list.setdefault(ns, []).append(resource)

    verbose("Resource list items:", ns_resource_list.items())

    # A list of namespaces that should be managed by tilt. Those namespaces will be managed under
    # a separate "cluster-bootstrap" resource/workload so that they are moved out of uncategorized.
    managed_namespaces = []
    # First, go over all applications and see if they can be rendered with tanka,
    # storing a list of all applications that were missing tanka environment.
    for namespace, apps in ns_resource_list.items():
        # Tilt doesn't create namespace automatically
        ns_config = NS_CONFIG.get(namespace, {})
        ns_labels = ns_config.get("labels", [])
        if ns_config.get("create_namespace", True):
            namespace_create(namespace, labels=ns_labels)
            managed_namespaces += [namespace + ":namespace"]

        ns_apps = DEP_TREE.get(namespace, {})
        for app in apps:
            app_config = ns_apps.get(app, {})
            tkenv = app_config.get("tkenv")
            if not tkenv:
                fail("Tanka app is not found for %s." % app)
            env_vars = app_config.get("env_vars", {})
            render_tanka(tkenv, env_vars)

    # Some resources are cluster-wide so we group them into a single workload "cluster-bootstrap",
    # to drop them from uncategorized resource list.
    cluster_bootstrap_resources = []

    # All namespaces created by tilt are kept in cluster-bootstrap
    if managed_namespaces:
        cluster_bootstrap_resources += managed_namespaces

    if cluster_bootstrap_resources:
        k8s_resource(objects=cluster_bootstrap_resources, new_name="cluster-bootstrap")

    for resource in resources:
        verbose("Registering resource", resource)
        resource_info = inv_dep_tree[resource]
        resource_name = resource_info.get("resource_name", resource)
        resource_labels = resource_info.get("labels", [])

        extra_objects = resource_info.get("extra_objects", [])
        ignored_objects = resource_info.get("ignored_objects", [])
        resource_deps = resource_info.get("resource_deps", [])
        if extra_objects:
            resolved_objects = resolve_objects(extra_objects, ignored_objects)
            k8s_resource(resource_name, objects=resolved_objects, labels=resource_labels, resource_deps=resource_deps)

        child_resources = resource_info.get("child_resources", [])
        for child in child_resources:
            extra_objects = []
            ignored_objects = []

            if "extra_objects" in child:
                extra_objects = child.pop("extra_objects")
            if "ignored_objects" in child:
                ignored_objects = child.pop("ignored_objects")
            if extra_objects:
                child["objects"] = resolve_objects(extra_objects, ignored_objects)

            child["labels"] = resource_labels
            verbose("Registering child", child)
            k8s_resource(**child)
        for kind in resource_info.get("kinds", []):
            k8s_kind(**kind)

    if "aperture-grafana" in resources:
        forward_port_delayed(
            workload="grafana-forward",
            namespace=APERTURE_CONTROLLER_NS,
            labels=["ApertureController"],
            resource_deps=["grafana"],
            service="aperture-grafana",
            local_port=3000,
            remote_port=3000,
            extra_env={
                "PERIOD": "1",
                "INITIAL_DELAY": "1",
            },
        )

    if "controller" in resources:
        readiness_probe=probe(initial_delay_secs=5, exec=exec_action(['kubectl', 'get', "svc", "aperture-controller", "-n", APERTURE_CONTROLLER_NS])) # Treat as success if service is available
        local_resource(
            name="controller-service",
            labels="ApertureController",
            resource_deps=["controller"],
            serve_cmd=['./scripts/check-controller-service.sh'],
            readiness_probe=readiness_probe,
        )

    if config.tilt_subcommand == "down":
        if "agent" in resources:
            local('if (kubectl get agent agent -n {0}); ret=$?; [ $ret -eq 0 ]; then kubectl delete agent agent -n {0};  fi'.format(APERTURE_AGENT_NS))
        if "controller" in resources:
            local('if (kubectl get controller controller -n {0}); ret=$?; [ $ret -eq 0 ]; then kubectl delete controller controller -n {0};  fi'.format(APERTURE_CONTROLLER_NS))
        if "policies" in resources:
            local('if (kubectl get policy service1-demo-app -n {0}); ret=$?; [ $ret -eq 0 ]; then kubectl delete policy service1-demo-app -n {0};  fi'.format(APERTURE_CONTROLLER_NS))
        for resource_name in resources:
            resource = inv_dep_tree[resource_name]
            pvc_selectors = resource.get("pvc_selectors", [])
            for selector in pvc_selectors:
                schedule_pvc_deletion(resource["namespace"], selector)

    if SKIP_LOCAL_CMDS:
        fail("SKIP_LOCAL_CMDS mode enabled")


############
### MAIN ###
############

# Parse config: https://docs.tilt.dev/tiltfile_config.html
config.define_string_list("to-run", args=True)
config.define_bool("list-resources", usage="Only list available resources and exit")
config.define_bool("manual", usage="Set trigger mode to manual")
config.define_bool("only", usage="Only deploy explicitly resources, without handling dependencies")
config.define_bool("verbose", usage="Enable verbose logging")
config.define_bool("trace", usage="Enable trace logging")
config.define_bool("race",usage="Enable race detector for agent and controller")
cfg = config.parse()
# Enable all resources, as calling `config.parse()` by default disables all
config.set_enabled_resources([])

if cfg.get("list-resources", False):
    print_available_resources(DEP_TREE)
    exit()

if cfg.get("manual", False):
    trigger_mode(TRIGGER_MODE_MANUAL)

get_race = cfg.get("race",False)

cfg_trace = cfg.get("trace", os.getenv("NINJA_TILT_TRACE", "false"))
TRACE = cfg_trace == True

cfg_verbose = cfg.get("verbose", os.getenv("NINJA_TILT_VERBOSE", cfg_trace))
VERBOSE = cfg_verbose == True

run_only_listed = cfg.get("only", False)
to_run = cfg.get("to-run", ["all"])

inverted_dep_tree = invert_dep_tree(DEP_TREE)
resources = get_resource_list_to_deploy(to_run, run_only_listed, DEP_TREE, inverted_dep_tree)

print("Will deploy resources:", ", ".join(resources))

# Watch charts and crd definitions
watch_file("Tiltfile")
watch_file("./tanka/apps/")
watch_file("./tanka/lib/")
watch_file("./tanka/charts/")
watch_file(GIT_ROOT + "/operator/config/crd")

run_wavepool_generator = '''
tk show --ext-str=projectRoot={0} {0}/apps/demoapp/ --target=configmap/wavepool-config --dangerous-allow-redirect | kubectl apply -f -
tk show --ext-str=projectRoot={0} {0}/apps/demoapp/ --target=deployment/wavepool-generator --dangerous-allow-redirect | kubectl apply -f -
'''.format(TANKA_ROOT)
cmd_button('wavepool-start',
    resource='wavepool-generator',
    icon_name='content_paste_go',
    text='Start Wavepool Generator',
    argv=['sh', '-c', run_wavepool_generator],
)

delete_wavepool_generator = '''
kubectl delete deploy wavepool-generator -n {0}
kubectl delete configmap wavepool-config -n {0}
'''.format(APERTURE_DEMOAPP_NS)
cmd_button('wavepool-stop',
    resource='wavepool-generator',
    icon_name='content_paste_off',
    text='Stop Wavepool Generator',
    argv=['sh', '-c', delete_wavepool_generator],
)

run_wavepool_graphql_generator = '''
tk show --ext-str=projectRoot={0} {0}/apps/demoapp/ --target=configmap/wavepool-graphql-config --dangerous-allow-redirect | kubectl apply -f -
tk show --ext-str=projectRoot={0} {0}/apps/demoapp/ --target=deployment/wavepool-graphql-generator --dangerous-allow-redirect | kubectl apply -f -
'''.format(TANKA_ROOT)
cmd_button('wavepool-graphql-start',
    resource='wavepool-graphql-generator',
    icon_name='content_paste_go',
    text='Start GraphQL Wavepool Generator',
    argv=['sh', '-c', run_wavepool_graphql_generator],
)

delete_wavepool_graphql_generator = '''
kubectl delete deploy wavepool-graphql-generator -n {0}
kubectl delete configmap wavepool-graphql-config -n {0}
'''.format(APERTURE_DEMOAPP_NS)
cmd_button('wavepool-graphql-stop',
    resource='wavepool-graphql-generator',
    icon_name='content_paste_off',
    text='Stop GraphQL Wavepool Generator',
    argv=['sh', '-c', delete_wavepool_graphql_generator],
)

aperture_go_pod_exec_script = '''
# get k8s pod name from tilt resource name
POD_NAME="$(tilt get kubernetesdiscovery "aperture-go-example" -ojsonpath='{.status.pods[0].name}')"
kubectl exec -n aperture-go-example "$POD_NAME" -- wget http://127.0.0.1:8080/super -O -
'''
cmd_button('aperture-go-trigger',
        argv=['sh', '-c', aperture_go_pod_exec_script],
        resource='aperture-go-example',
        icon_name='ads_click',
        text='Trigger library example',
)

aperture_js_pod_exec_script = '''
# get k8s pod name from tilt resource name
POD_NAME="$(tilt get kubernetesdiscovery "aperture-js-example" -ojsonpath='{.status.pods[0].name}')"
kubectl exec -n aperture-js-example "$POD_NAME" -- wget http://127.0.0.1:8080/super -O -
'''
cmd_button('aperture-js-trigger',
        argv=['sh', '-c', aperture_js_pod_exec_script],
        resource='aperture-js-example',
        icon_name='ads_click',
        text='Trigger library example',
)

aperture_java_pod_exec_script = '''
# get k8s pod name from tilt resource name
POD_NAME="$(tilt get kubernetesdiscovery "aperture-java-example" -ojsonpath='{.status.pods[0].name}')"
kubectl exec -n aperture-java-example "$POD_NAME" -- wget http://127.0.0.1:8080/super -O -
'''
cmd_button('aperture-java-trigger',
        argv=['sh', '-c', aperture_java_pod_exec_script],
        resource='aperture-java-example',
        icon_name='ads_click',
        text='Trigger library example',
)
declare_resources(resources, DEP_TREE, inverted_dep_tree,get_race)
