# vim: ft=bzl sw=4
v1alpha1.extension_repo(name='default', url='https://github.com/tilt-dev/tilt-extensions', ref='v0.31.2')
load("ext://namespace", "namespace_create", "namespace_inject")
load('ext://uibutton', 'cmd_button', 'location')

allow_k8s_contexts('gke_devel-309501_us-central1_kklimonda-dev-cluster')

#################
### CONSTANTS ###
#################
IMG_REPO = "docker.io/fluxninja"
GIT_ROOT = str(local("git rev-parse --show-toplevel")).strip()
PLAYGROUND_ROOT = GIT_ROOT + "/playground"
TANKA_ROOT = GIT_ROOT + "/playground/tanka"
TILT_OBJECTS = []
APERTURE_GO_REPO_PATH = GIT_ROOT + "/sdks/aperture-go"
APERTURE_JAVA_REPO_PATH = GIT_ROOT + "/sdks/aperture-java"
APERTURE_JS_REPO_PATH = GIT_ROOT + "/sdks/aperture-js"
APERTURE_AGENT_NS = "aperture-agent"
APERTURE_CONTROLLER_NS = "aperture-controller"
APERTURE_GO_EXAMPLE_NS = "aperture-go-example"
APERTURE_JS_EXAMPLE_NS = "aperture-js-example"
APERTURE_JAVA_EXAMPLE_NS = "aperture-java-example"
APERTURE_DEMOAPP_NS = "demoapp"
APERTURE_ISTIO_NS = "istio-system"
DEFAULT_TEST_SCENARIO = "scenarios/rate_limiting_escalation"
TANKA = "tk"
SKIP_LOCAL_CMDS = False  # If set to true, skips actually running local commands (useful for developing and testing Tiltfile itself)
TANKA_DEPS_HANDLED = []
DASHBOARDS = []

# Dependency tree and service configuration.
# Top level maps NAMESPACE to services
# Services are a mapping of service name to their config, which consists of:
# "labels": A list of labels to apply to the service
# "needs": list of services (from any namespace) which need to be brought up for the service to work
# "tkenv": Path to
# "images": list of dicts, defining arguments passed to build_docker_img. Arguments are:
## "tag": Docker image tag. If specified, image is pulled, else, it is built
## "ref": Docker image name
## "live_update": Live update callback
## "context": Context to build the image in
## "dockerfile": Location of Dockerfile to build the image from
## "ssh": Unused. TODO: Remove
# "port_forwards": any value accepted by k8s_resource's port_forwards argument. In addition to that, it can be a dict describing multiple port_forwards (mapping name to port_forwards, as above)
# "child_resources"
# "pvc_selectors"
# "kinds"
DEP_TREE = {
    APERTURE_CONTROLLER_NS: {
        "aperture-grafana": {
            "labels": ["ApertureController"],
            # construct directory path to tkenv
            "tkenv": TANKA_ROOT + "/apps/aperture-grafana",
            "child_resources": [
                {
                    "new_name": "grafana-operator-bootstrap",
                    "extra_objects": [
                        ("integreatly.org$", "CustomResourceDefinition", None),
                    ]
                },
                {
                    "workload": "aperture-grafana-operator",
                    "new_name": "grafana-operator",
                    "resource_deps": ["grafana-operator-bootstrap"],
                    "extra_objects": [
                        ("^aperture-grafana-operator", None, APERTURE_CONTROLLER_NS)
                    ],
                    "ignored_objects": [
                        ("^aperture-grafana-operator", "Deployment", APERTURE_CONTROLLER_NS)
                    ]
                },
                {
                    "new_name": "grafana",
                    "resource_deps": ["grafana-operator"],
                    "extra_objects": [
                        "aperture-grafana:grafana",
                        "controller-prometheus:grafanadatasource"
                    ]
                },
                {
                    "new_name": "dashboards",
                    "resource_deps": ["grafana-operator-bootstrap"],
                    "extra_objects": [
                        (None, "GrafanaDashboard", APERTURE_CONTROLLER_NS)
                    ]
                },
                {
                    "new_name": "grafana-data-sources",
                    "resource_deps": ["grafana-operator-bootstrap"],
                    "extra_objects": [
                        (None, "GrafanaDataSource", APERTURE_CONTROLLER_NS)
                    ]
                }
            ]
        },
        "controller": {
           "labels": ["ApertureController"],
            "tkenv": TANKA_ROOT + "/apps/aperture-controller",
            "images": [
                {
                    "ref": "aperture-controller",
                    "context": GIT_ROOT,
                    "dockerfile": GIT_ROOT + "/cmd/aperture-controller/Dockerfile",
                    "ssh": "default",
                    "build_args":{'RACE': ""},
                },
                {
                    "ref": "aperture-operator",
                    "context": GIT_ROOT,
                    "dockerfile": GIT_ROOT + "/operator/Dockerfile",
                    "ssh": "default",
                }
            ],
            "port_forwards": {
                "controller-prometheus-server": "9090:9090",
                "controller-etcd": "2379:2379",
            },
            "pvc_selectors": ["app.kubernetes.io/name=etcd"],
            "kinds": [
                dict(kind="Controller", image_object={'json_path': '{.spec.image}', 'repo_field': 'repository', 'tag_field': 'tag'}, pod_readiness='wait'),
            ],
            "child_resources": [
                {
                    "new_name": "controller-operator-bootstrap",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        "controllers.fluxninja.com:customresourcedefinition",
                        "policies.fluxninja.com:customresourcedefinition",
                        "aperture-controller-defaulter:mutatingwebhookconfiguration",
                    ]
                },
                {
                    "workload": "controller",
                    "new_name": "controller",
                    "resource_deps": ["controller-etcd", "controller-aperture-controller-manager", "controller-prometheus"],
                    "extra_pod_selectors": [
                        {'app.kubernetes.io/component':  'controller', 'app.kubernetes.io/managed-by': 'aperture-operator'},
                    ]
                },
                {
                    "workload": "controller-aperture-controller-manager",
                    "resource_deps": ["controller-operator-bootstrap"],
                    "extra_objects": [
                        ("^controller-aperture-controller", "Role|RoleBinding|ServiceAccount", APERTURE_CONTROLLER_NS),
                        ("^controller-aperture-controller", "ClusterRole|ClusterRoleBinding"),
                    ],
                    "ignored_objects": [
                        ("^controller-aperture-controller-manager", "Service|Deployment", APERTURE_CONTROLLER_NS),
                    ]
                },
                {
                    "workload": "controller-etcd",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        ("^controller-etcd", None),
                    ],
                    "ignored_objects": [
                        ("^controller-etcd", "Service|StatefulSet")
                    ]
                },
                {
                    "workload": "controller-prometheus-server",
                    "new_name": "controller-prometheus",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        ("^controller-prometheus", None),
                    ],
                    "ignored_objects": [
                        ("^controller-prometheus", "Service|Deployment"),
                        "controller-prometheus:grafanadatasource"
                    ]
                },
                {
                    "workload": "controller-kube-state-metrics",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        ("^controller-kube-state-metrics", None),
                    ],
                    "ignored_objects": [
                        ("^controller-kube-state-metrics", "Service|Deployment", APERTURE_CONTROLLER_NS),
                    ]
                },
            ]
        }
    },
    APERTURE_AGENT_NS: {
        "agent": {
            "labels": ["ApertureAgent"],
            "needs": ["controller"],
            "tkenv": TANKA_ROOT + "/apps/aperture-agent",
            "images": [
                {
                    "ref": "aperture-agent",
                    "context": GIT_ROOT,
                    "dockerfile": GIT_ROOT + "/cmd/aperture-agent/Dockerfile",
                    "ssh": "default",
                    "build_args":{'RACE':""},
                }
            ],
            "port_forwards": {
                "agent": "8080:8080",
            },
            "kinds": [
                dict(kind="Agent", image_object={'json_path': '{.spec.image}', 'repo_field': 'repository', 'tag_field': 'tag'}, pod_readiness='wait'),
            ],
            "child_resources": [
                {
                    "new_name": "agent-operator-bootstrap",
                    "resource_deps": ["cluster-bootstrap"],
                    "extra_objects": [
                        "agents.fluxninja.com:customresourcedefinition",
                         "aperture-agent-defaulter:mutatingwebhookconfiguration",
                    ]
                },
                {
                    "workload": "agent",
                    "new_name": "agent",
                    "resource_deps": ["controller-etcd", "agent-aperture-agent-manager", "controller-prometheus"],
                    "extra_pod_selectors": [
                        {'app.kubernetes.io/component':  'agent', 'app.kubernetes.io/managed-by': 'aperture-operator'},
                    ]
                },
                {
                    "workload": "agent-aperture-agent-manager",
                    "resource_deps": ["agent-operator-bootstrap"],
                    "extra_objects": [
                        ("^agent-aperture-agent", "Role|RoleBinding|ServiceAccount", APERTURE_AGENT_NS),
                        ("^agent-aperture-agent", "ClusterRole|ClusterRoleBinding"),
                    ],
                    "ignored_objects": [
                        ("^agent-aperture-agent-manager", "Service|Deployment", APERTURE_AGENT_NS),
                    ]
                }
            ]
        }
    },
    APERTURE_ISTIO_NS: {
        "istio": {
            "labels": ["Istio"],
            "needs": ["controller", "agent"],
            "tkenv": TANKA_ROOT + "/apps/istio",
            "child_resources": [
                {
                    "new_name": "istio-bootstrap",
                    "extra_objects": [
                        "istio:configmap",
                        "aperture-envoy-filter:envoyfilter",
                        "istiod-default-validator:validatingwebhookconfiguration",
                        ("istio.io$", "customresourcedefinition"),
                        (None, "EnvoyFilter", "istio-system"),
                        ("^istio-reader", None),
                        ("^istio-sidecar", None),
                    ]
                },
                {
                    "workload": "istiod",
                    "resource_deps": ["istio-bootstrap"],
                    "extra_objects": [
                        ("^istiod", None),
                    ],
                    "ignored_objects": [
                        ("^istiod", "Service|Deployment")
                    ]
                },
                {
                    "workload": "gateway",
                    "resource_deps": ["istio-bootstrap", "istiod"],
                    "extra_objects": [
                        ("^gateway", None),
                    ],
                    "ignored_objects": [
                        ("^gateway", "Service|Deployment"),
                    ]
                },
            ]
        },
    },
    APERTURE_GO_EXAMPLE_NS: {
        "aperture-go-example": {
            "labels": ["Aperture"],
            "needs": ["agent"],
            "tkenv": TANKA_ROOT + "/apps/aperture-go-example",
            "images": [
                {
                    "ref": "aperture-go-example",
                    "context": APERTURE_GO_REPO_PATH,
                }
            ],
            "child_resources": [
                {
                    "workload": "aperture-go-example",
                    "resource_deps": ["agent", "controller", "cluster-bootstrap"],
                }
            ]
        },
    },
    APERTURE_JS_EXAMPLE_NS: {
        "aperture-js-example": {
            "labels": ["Aperture"],
            "needs": ["agent"],
            "tkenv": TANKA_ROOT + "/apps/aperture-js-example",
            "images": [
                {
                    "ref": "aperture-js-example",
                    "context": APERTURE_JS_REPO_PATH if os.path.exists(APERTURE_JS_REPO_PATH) else "https://github.com/fluxninja/aperture-js.git#main",
                }
            ],
            "child_resources": [
                {
                    "workload": "aperture-js-example",
                    "resource_deps": ["agent", "controller", "cluster-bootstrap"],
                }
            ]
        },
    },
    APERTURE_JAVA_EXAMPLE_NS: {
        "aperture-java-example": {
            "labels": ["Aperture"],
            "needs": ["agent"],
            "tkenv": TANKA_ROOT + "/apps/aperture-java-example",
            "images": [
                {
                    "ref": "aperture-java-example",
                    "context": APERTURE_JAVA_REPO_PATH,
                    "dockerfile": "dockerfiles/example/Dockerfile",
                }
            ],
            "child_resources": [
                {
                    "workload": "aperture-java-example",
                    "resource_deps": ["agent", "controller", "cluster-bootstrap"],
                }
            ]
        },
    },
}

NS_CONFIG = {
    APERTURE_DEMOAPP_NS: {
        "labels": ["istio-injection: enabled"],
    }
}

###############
### HELPERS ###
###############

def verbose(*msg):
    if VERBOSE:
        print("VERBOSE:", *msg)

def trace(*msg):
    if TRACE:
        print("TRACE:", *msg)

def merge_dicts(*dicts):
    merged = {}
    for d in dicts:
        merged.update(d)
    return merged

def build_remote_docker_img(ref, context, **kwargs):
    if kwargs:
        fail("build_remote_docker_img doesn't support kwargs: {}".format(kwargs))
    custom_build(
        ref=ref,
        deps=[],
        # Would be better to run as list[str], but then we can't reference the env var
        command='docker build -t "$EXPECTED_REF" "{context}"'.format(context=context),
    )

def build_docker_img(ref, context, dockerfile="Dockerfile", **kwargs):
    # Wrapper around docker_build to not repeat image repo,
    print(
        "Will build",
        ref,
        "with context of",
        context,
        "using",
        dockerfile,
        "and",
        kwargs,
    )
    ref = IMG_REPO.rstrip("/") + "/" + ref
    if context.startswith("https://"):
        build_remote_docker_img(ref, context, **kwargs)
    else:
        if not context.startswith("/"):
            context = GIT_ROOT + "/" + context
        if not dockerfile.startswith("/"):
            dockerfile = context + "/" + (dockerfile or "Dockerfile")
        docker_build(ref, context, dockerfile=dockerfile, **kwargs)


def pull_docker_img(ref, tag):
    print("Will pull", ref, "with version", tag)
    ref = IMG_REPO.rstrip("/") + "/" + ref
    custom_build(
      ref=ref,
      command=["docker", "pull", ref + ":" + tag],
      deps=[],
      tag=tag,
    )


def _convert_yaml_into_objects(yaml):
    if type(yaml) == "string":
        return read_yaml_stream(yaml)
    elif type(yaml) == "blob":
        return decode_yaml_stream(yaml)
    else:
        fail('only takes string or blob, got: %s' % type(yaml))


def update_tilt_objects(objects):
    for obj in objects:
        object_name = obj["metadata"]["name"].lower()
        object_name = object_name.replace(":", "\\:")

        object_kind = obj["kind"].lower()
        object_namespace = obj["metadata"].get("namespace")

        object_selector = ":".join([object_name, object_kind])
        if object_namespace:
            object_selector += ":%s" % object_namespace

        TILT_OBJECTS.append(object_selector)


def wrapped_k8s_yaml(yaml, allow_duplicates=False):
    objects = _convert_yaml_into_objects(yaml)
    update_tilt_objects(objects)

    k8s_yaml(yaml, allow_duplicates)


def refresh_chart_dependencies(chart_dir):
    dep_output = local(command=["helm", "dependency", "list"], dir=chart_dir)
    lines = str(dep_output).splitlines()
    for number in range(1, len(lines) - 1):
        if not lines[number].strip().endswith("ok"):
            local(command=["helm", "dependency", "update"], dir=chart_dir)
            return

def render_tanka(environment, env_vars, base_dir, namespace, enable_cloud_plugin):
    print(
        "Rendering tanka environment",
        environment,
    )

    if base_dir == None:
        base_dir = environment.split("apps/")[0] or "./"

    if SKIP_LOCAL_CMDS:
        return

    if base_dir not in TANKA_DEPS_HANDLED:
        # Get base directory for Jsonnet Bundler execution
        # tanka/apps/default -> tanka/
        # apps/default -> ./
        # Make sure jb dependencies are up to date
        local(command=["./scripts/deps_install.sh", base_dir])

        # Skip refreshing deps the second time
        TANKA_DEPS_HANDLED.append(base_dir)

    trace("Rendering tanka with env: %s" % env_vars)

    enable_cloud_plugin = str(enable_cloud_plugin)
    # Generate manifests with tanka
    rendered = local(
        quiet=True,
        command=[
            TANKA,
            "--ext-str=projectRoot=" + base_dir,
            "--ext-str=ENABLE_CLOUD_PLUGIN=" + enable_cloud_plugin,
            "show",
            # Allow dumping the templates to stdout,
            # instead of using tk export to generate files
            # and then have to manage deleting them etc.
            "--dangerous-allow-redirect",
            environment
        ],
        env=env_vars
    )

    if namespace:
        rendered = namespace_inject(rendered, namespace)
    wrapped_k8s_yaml(rendered)


def render_dashboards(policies):
    aperturectl_bin = str(local(command=["../scripts/build_aperturectl.sh"], quiet=True))

    blueprints_uri = os.path.join(GIT_ROOT, "blueprints")
    dashboards = {}
    base_dir = os.path.join(GIT_ROOT, "playground")
    for policy in policies:
        blueprint_name = policy["blueprint_name"]
        policy_name = policy["policy_name"]
        values_file = policy["values_file"]
        dashboard_mixin_dir = policy["dashboard_mixin_dir"]
        rendered = local(command=["./scripts/render-dashboard.sh", base_dir, aperturectl_bin, blueprints_uri,
                                    blueprint_name, policy_name, values_file], quiet=True)
        dashboards[policy_name] = {"path": dashboard_mixin_dir, "content": rendered}

    return dashboards


def render_aperturectl(policies, namespace):
    aperturectl_bin = str(local(command=["../scripts/build_aperturectl.sh"], quiet=True))

    blueprints_uri = os.path.join(GIT_ROOT, "blueprints")
    base_dir = os.path.join(GIT_ROOT, "playground")
    for policy in policies:
        blueprint_name = policy["blueprint_name"]
        policy_name = policy["policy_name"]
        values_file = policy["values_file"]
        rendered = local(command=["./scripts/render-policy.sh", base_dir, aperturectl_bin, blueprints_uri,
                                  blueprint_name, policy_name, values_file], quiet=True)

        if namespace:
            rendered = namespace_inject(rendered, namespace)
        wrapped_k8s_yaml(rendered)


def print_available_resources(dep_tree):
    print("Following resources are available:")
    for ns, resources in dep_tree.items():
        print(ns)
        for resource in resources.keys():
            print("   ", resource)


def schedule_pvc_deletion(namespace, selector):
    # Cleanup DB
    # Marks PVC for deletion, but doesn't wait since finalizers need to run first
    # and they will be ran later when tilt deletes created resources
    local(
        command=[
            "kubectl",
            "--namespace",
            namespace,
            "delete",
            "pvc",
            "--selector",
            selector,
            "--wait=false",
        ]
    )

#####################
### OPTION PARSER ###
#####################


def invert_dep_tree(dep_tree):
    """Returns dict of resources, with injected namespace into resource dict"""
    inverted = {}
    for ns, resources in dep_tree.items():
        for resource, resource_info in resources.items():
            info_copy = dict(resource_info)
            info_copy["namespace"] = ns
            inverted[resource] = info_copy
    return inverted


def get_resource_list_to_deploy(names, run_only_listed, dep_tree, inv_dep_tree):
    """
    Recursively convert list of NS and service names to set of required services
    If a given name is both a name of resource and namespace, it is assumed to be resource
    """
    if "all" in names:
        names = inv_dep_tree.keys()
    resources = (
        []
    )  # It'd be faster and better to use set here, but Starlark set only has .union method
    for name in names:
        if name in inv_dep_tree:  # It's resource name
            resources.append(name)
            if not run_only_listed:
                # We could recursively resolve dependencies, but then we risk getting in a dependency loop
                resource_info = inv_dep_tree[name]
                dependencies = resource_info.get("needs", [])
                while len(dependencies) > 0:
                    dep = dependencies.pop()
                    if dep not in resources:
                        resources.append(dep)
                        dependencies.extend(inv_dep_tree[dep].get("needs", []))
        elif name in dep_tree:  # It's a namespace
            resource_names = dep_tree[name].keys()
            resources.extend(
                get_resource_list_to_deploy(resource_names, run_only_listed, dep_tree, inv_dep_tree)
            )
        else:
            fail("Unable to find " + name + " as neither resource nor namespace name")
    return set(resources)


def parse_kubernetes_object_selector(selector):
    """Parse string representing kubernetes object selector into its parts

    Tilt defines kubernetes object selectors as name[:kind[:namespace]], and kubernetes
    names can contain colons. This function parses given selector into its parts, while
    making sure that name with colons is properly handled.
    """
    parts = selector.split(":")

    name, kind, namespace = "", None, None
    # First, get name from parts - names in kubernetes may contain : so we must
    # rebuild it here, probably from multiple parts.
    while True:
        name += parts.pop(0).lower()
        if not name.endswith("\\"):
            break
        name = name[:-1] + ":"
        if not parts:
            fail("Unable to parse selector '%s'" % selector)

    if parts:
        kind = parts.pop(0).lower()

    if parts:
        namespace = parts.pop(0).lower()

    if parts:
        fail("Unable to parse selector '%s'" % selector)

    return (name, kind, namespace)


MATCHER_MATCH_START = 1
MATCHER_MATCH_END = 2
MATCHER_MATCH_FULL = 3
MATCHER_MATCH_EMPTY = 4
def match_kubernetes_object_selector_with_glob(selector_glob):
    """Returns all kubernetes objects known by tilt that match given matcher and string

    Starlark has no regex engine, so to get all matching objects we must cheat a bit. This
    function requires matcher and kind arguments, both strings: kind is lowercase kubernetes
    kind, and matcher is a string starting with ^ or ending with $, converting them into
    startswith() and endwith() respectively.
    """
    matcher_name = selector_glob[0]
    matcher_kind = selector_glob[1]
    if len(selector_glob) == 3:
        matcher_namespace = selector_glob[2]
    else:
        matcher_namespace = None

    if not matcher_name:
        matcher_type = MATCHER_MATCH_EMPTY
    elif matcher_name[0] == "^" and matcher_name[-1] == "$":
        matcher_name = matcher_name[1:-1]
        matcher_type = MATCHER_MATCH_FULL
    elif matcher_name[0] == "^":
        matcher_name = matcher_name[1:]
        matcher_type = MATCHER_MATCH_START
    elif matcher_name[-1] == "$":
        matcher_name = matcher_name[:-1]
        matcher_type = MATCHER_MATCH_END
    else:
        fail("Need to specify starting or ending matcher token for '%s'" % (matcher_name))
    trace("Running glob object selector with name=%s, kind=%s and namespace=%s" % (matcher_name, matcher_kind, matcher_namespace))

    matched_objs = []
    for obj in TILT_OBJECTS:
        name, kind, namespace = parse_kubernetes_object_selector(obj)

        if matcher_namespace and matcher_namespace != namespace:
            continue

        if matcher_kind:
            kinds = [k.lower() for k in matcher_kind.split("|")]
            if kind not in kinds:
                continue

        if matcher_type == MATCHER_MATCH_START and name.startswith(matcher_name):
            matched_objs.append(obj)
        elif matcher_type == MATCHER_MATCH_END and name.endswith(matcher_name):
            matched_objs.append(obj)
        elif matcher_type == MATCHER_MATCH_FULL and name == matcher_name:
            matched_objs.append(obj)
        elif matcher_type == MATCHER_MATCH_EMPTY:
            matched_objs.append(obj)

    return matched_objs


def match_kubernetes_object_selector(selector):
    """Find tilt object matching given kubernetes object selector

    Given a valid selector, find a matching object in a list of objects known
    to tilt. Fail if more than one object matches the given selector.
    """
    matched_obj = None
    parsed_selector = parse_kubernetes_object_selector(selector)

    for tilt_obj in TILT_OBJECTS:
        parsed_object = parse_kubernetes_object_selector(tilt_obj)
        parsed_object_without_ns = (parsed_object[0], parsed_object[1], None)
        # If selector matches given object (check both with and without namespace as selectors may be partial),
        # keep matched object reference and continue iterating the list to see if selector matches multiple
        # objects, which is an error.
        if parsed_selector == parsed_object or parsed_selector == parsed_object_without_ns:
            if matched_obj:
                fail("Multiple tilt objects matched selector '%s'" % (selector))
            matched_obj = tilt_obj

    return matched_obj


def _resolve_objects_match_objects(matchers, fail_on_missing=False):
    """Helper used for creating a list of all tilt objects matching matchers"""
    matched_objs = []
    for matcher in matchers:
        if type(matcher) == "string":
            matched = match_kubernetes_object_selector(matcher)
            if not matched and fail_on_missing:
                fail("Found no match for selector '%s'" % matcher)
            elif matched and matched not in matched_objs:
                matched_objs += [matched]
        else:
            matched = match_kubernetes_object_selector_with_glob(matcher)
            matched_not_present = [match for match in matched if match not in matched_objs]
            matched_objs.extend(matched_not_present)
    return matched_objs


def resolve_objects(included_matchers, ignored_matchers):
    """Resolve all objects from included_matchers into tilt objects

    For the included_matches list, get all objects from TILT_OBJECTS that are
    matching items of this list, and filter out objects that match items of the
    ignored_matchers list. This is useful when you want to use wide includes, and
    then exclude some subset of the matched items.
    """
    matched_objs = _resolve_objects_match_objects(included_matchers, fail_on_missing=True)
    verbose("Include matchers found", matched_objs)
    ignored_objs = _resolve_objects_match_objects(ignored_matchers)
    verbose("Ignore matchers found", ignored_objs)

    for obj in ignored_objs:
        if obj in matched_objs:
            matched_objs.remove(obj)

    for obj in matched_objs:
        TILT_OBJECTS.remove(obj)

    return matched_objs


def render_yaml(manifests_path, env_vars=None):
    if not env_vars:
        env_vars = {}

    wrapped_k8s_yaml(
        local(
            quiet=True,
            command=[
                "./scripts/render-yaml.sh",
                manifests_path
            ],
            env=env_vars
        )
    )


def render_jsonnet(manifests_path, jsonnet_args, namespace):
    if not jsonnet_args:
        jsonnet_args = []

    rendered = decode_json(str(local(
        quiet=True,
        command=["jsonnet"] + jsonnet_args + [manifests_path]
    )))

    def to_yaml(obj):
        obj = blob(str(obj))
        if namespace:
            obj = namespace_inject(obj, namespace)
        wrapped_k8s_yaml(obj)

    if type(rendered) == "list":
        for obj in rendered:
            to_yaml(obj)
    else:
        to_yaml(rendered)


def process_aperture_scenario(scenario_path):
    # If this is a relative path, it's relative to Tiltfile (PLAYGROUND_ROOT_PATH)
    if scenario_path[0] != "/":
        scenario_path = PLAYGROUND_ROOT + "/" + scenario_path

    scenario_metadata = read_json(scenario_path + "/metadata.json", default='')
    if scenario_metadata == '':
        fail("Unable to find or parse metadata.json, will not continue.")

    if "tkenv" in scenario_metadata:
        # tkenv is relative to GIT_ROOT
        scenario_metadata["tkenv"] = GIT_ROOT + "/" + scenario_metadata["tkenv"]

    scenario_metadata["labels"] = "TestScenarioApplication"

    images = scenario_metadata.get("images", [])
    # image context are relative to GIT_ROOT
    for image in images:
        if "context" in image:
            image["context"] = GIT_ROOT + "/" + image["context"]

    if APERTURE_DEMOAPP_NS not in DEP_TREE:
        DEP_TREE[APERTURE_DEMOAPP_NS] = {}
    DEP_TREE[APERTURE_DEMOAPP_NS]["scenario-app"] = scenario_metadata

    aperture_policies = scenario_metadata.pop("aperture_policies", [])
    # policy values_file are relative to the scenario path
    for policy in aperture_policies:
        if "values_file" in policy:
            policy["values_file"] = scenario_path + "/" + policy["values_file"]
        if "dashboard_mixin_dir" in policy:
            policy["dashboard_mixin_dir"] = scenario_path + "/" + policy["dashboard_mixin_dir"]

    if aperture_policies:
        policies = {
            "renderer": "aperturectl",
            "namespace": APERTURE_CONTROLLER_NS,
            "labels": "TestScenarioApplication",
            "scenario_path": scenario_path,
            "aperture_policies": aperture_policies,
            "resource_deps": ["scenario-app"],
            "child_resources": [
                {
                    "new_name": "scenario-policies",
                    "resource_deps": ["controller-service"],
                    "extra_objects": [
                        (None, "Policy", APERTURE_CONTROLLER_NS)
                    ]
                }
            ]
        }
        DEP_TREE[APERTURE_DEMOAPP_NS]["scenario-policies"] = policies

        for policy_name, dashboard in render_dashboards(aperture_policies).items():
            ds = {
                "renderer": "jsonnet",
                "namespace": APERTURE_CONTROLLER_NS,
                "labels": "ApertureController",
                "needs": ["aperture-grafana"],
                "manifests_path": os.path.join(dashboard["path"], "main.jsonnet"),
                "jsonnet_args": ["-J", PLAYGROUND_ROOT + "/tanka/vendor",
                                 "--ext-str", "APERTURE_DASHBOARD=%s" % dashboard["content"],
                                 "--ext-str", "POLICY_NAME=%s" % policy_name],
            }
            DEP_TREE[APERTURE_CONTROLLER_NS]["scenario-dashboard-%s" % policy_name] = ds


    # Load k6 configuration and process it for inclusion into a yaml file
    wavepool_js = str(read_file(scenario_path + "/" + "load_generator/test.js"))
    lines = wavepool_js.replace("\"", "\\\"").split("\n")
    wavepool_js = "\\n".join(lines)
    wavepool = {
        "renderer": "yaml",
        "namespace": APERTURE_DEMOAPP_NS,
        "labels": "TestScenarioApplication",
        "manifests_path": PLAYGROUND_ROOT + "/" + "resources/wavepool-generator",
        "resource_deps": ["scenario-policies", "cluster-bootstrap"],
        "env_vars": {
            "NINJA_WAVEPOOL_JS": wavepool_js
        },
        "child_resources": [
            {
                "workload": "wavepool-generator",
                "resource_deps": ["scenario-policies"] if aperture_policies else [],
                "extra_objects": [
                    "wavepool-config:configmap"
                ]
            },
        ]
    }
    DEP_TREE[APERTURE_DEMOAPP_NS]["scenario-wavepool"] = wavepool


#################################
### MAIN RESOURCE DECLARATION ###
#################################


def forward_port_delayed(workload, labels, resource_deps, namespace, service, local_port, remote_port, extra_env={}, readiness_probe=None):
    if not readiness_probe:
        readiness_probe=probe(exec=exec_action(['lsof', '-i', ":%s" % local_port])) # Treat as success if some process listens on this port
    local_resource(
        name=workload,
        labels=labels,
        resource_deps=resource_deps,
        serve_cmd=['./scripts/portforward.sh'],
        serve_env=merge_dicts(
            {
                "NAMESPACE": namespace,
                "RESOURCE_NAME": service,
                "LOCAL_PORT": str(local_port),
                "REMOTE_PORT": str(remote_port),
            },
            extra_env,
        ),
        readiness_probe=readiness_probe,
    )


def declare_resources(resources, dep_tree, inv_dep_tree,race_arg,enable_cloud_plugin):
    """
    Declares resource-specific actions (images, ports, etc)
    Generating manifests with tanka is done separately, in order to speed up builds
    """
    for resource in resources:
        resource_info = inv_dep_tree[resource]
        labels = resource_info.get("labels", [])
        if race_arg:
            images = resource_info.get("images",{})
            if resource == "controller":
                images[0]["build_args"]["RACE"] = "Yes"
            if resource == "agent":
                images[0]["build_args"]["RACE"] = "Yes"
        for image in resource_info.get("images", []):
            if 'tag' in image.keys():
                # We will be pulling existing image
                if not set(image.keys()) == set(['tag', 'ref']):
                    fail("Invalid image configuration: {img}".format(img=image))
                pull_docker_img(**image)
            else:
                # We build images depending on what we want to deploy, and so live_update
                # steps have to be a lambda, so that they are only created when we realize
                # the image.
                live_update_callable = image.get("live_update")
                if live_update_callable:
                    image["live_update"] = live_update_callable()
                build_docker_img(**image)

        pf = resource_info.get("port_forwards", None)

        if pf:
            if type(pf) == "dict":
                # We have multiple pf resources to define
                for pf_name, pf_value in pf.items():
                    k8s_resource(pf_name, labels=labels, port_forwards=pf_value)
            else:
                k8s_resource(resource, labels=labels, port_forwards=pf)

    # Create mapping of namespace to list of resources within the namespace
    ns_resource_list = {}
    for resource in resources:
        resource_info = inv_dep_tree[resource]
        ns = resource_info["namespace"]
        # setdefault is a bit misleading:
        # it gets value for given key, if it exists
        # if it doesn't, it puts the default value under that key
        # and returns the selected value
        ns_resource_list.setdefault(ns, []).append(resource)

    verbose("Resource list items:", ns_resource_list.items())

    # A list of namespaces that should be managed by tilt. Those namespaces will be managed under
    # a separate "cluster-bootstrap" resource/workload so that they are moved out of uncategorized.
    managed_namespaces = []
    # First, go over all applications and see if they can be rendered with tanka,
    # storing a list of all applications that were missing tanka environment.
    for namespace, apps in ns_resource_list.items():
        # Tilt doesn't create namespace automatically
        ns_config = NS_CONFIG.get(namespace, {})
        ns_labels = ns_config.get("labels", [])
        if ns_config.get("create_namespace", True):
            namespace_create(namespace, labels=ns_labels)
            managed_namespaces += [namespace + ":namespace"]

        ns_apps = DEP_TREE.get(namespace, {})
        for app in apps:
            app_config = ns_apps.get(app, {})
            renderer = app_config.get("renderer")
            manifests_path = app_config.get("manifests_path")
            tkenv = app_config.get("tkenv")
            env_vars = app_config.get("env_vars", {})
            namespace = app_config.get("namespace", None)
            if tkenv:
                base_dir = app_config.get("base_dir")
                render_tanka(tkenv, env_vars, base_dir, namespace, enable_cloud_plugin)
            elif renderer == "yaml":
                render_yaml(manifests_path, env_vars)
            elif renderer == "jsonnet":
                namespace = app_config.get("namespace", None)
                jsonnet_args = app_config.get("jsonnet_args", [])
                render_jsonnet(manifests_path, jsonnet_args, namespace)
            elif renderer == "aperturectl":
                namespace = app_config.get("namespace", None)
                policies = app_config.get("aperture_policies", None)
                render_aperturectl(policies, namespace)
            else:
                fail("Tanka app is not found for %s." % app)

    # Some resources are cluster-wide so we group them into a single workload "cluster-bootstrap",
    # to drop them from uncategorized resource list.
    cluster_bootstrap_resources = []

    # All namespaces created by tilt are kept in cluster-bootstrap
    if managed_namespaces:
        cluster_bootstrap_resources += managed_namespaces

    if cluster_bootstrap_resources:
        k8s_resource(objects=cluster_bootstrap_resources, new_name="cluster-bootstrap")

    for resource in resources:
        verbose("Registering resource", resource)
        resource_info = inv_dep_tree[resource]
        resource_name = resource_info.get("resource_name", resource)
        resource_labels = resource_info.get("labels", [])

        extra_objects = resource_info.get("extra_objects", [])
        ignored_objects = resource_info.get("ignored_objects", [])
        resource_deps = resource_info.get("resource_deps", [])
        if extra_objects:
            resolved_objects = resolve_objects(extra_objects, ignored_objects)
            k8s_resource(resource_name, objects=resolved_objects, labels=resource_labels, resource_deps=resource_deps)

        child_resources = resource_info.get("child_resources", [])
        for child in child_resources:
            extra_objects = []
            ignored_objects = []

            if "extra_objects" in child:
                extra_objects = child.pop("extra_objects")
            if "ignored_objects" in child:
                ignored_objects = child.pop("ignored_objects")
            if extra_objects:
                child["objects"] = resolve_objects(extra_objects, ignored_objects)

            child["labels"] = resource_labels
            verbose("Registering child", child)
            k8s_resource(**child)
        for kind in resource_info.get("kinds", []):
            k8s_kind(**kind)

    if "aperture-grafana" in resources:
        forward_port_delayed(
            workload="grafana-forward",
            namespace=APERTURE_CONTROLLER_NS,
            labels=["ApertureController"],
            resource_deps=["grafana"],
            service="aperture-grafana",
            local_port=3000,
            remote_port=3000,
            extra_env={
                "PERIOD": "1",
                "INITIAL_DELAY": "1",
            },
        )

    if "controller" in resources:
        readiness_probe=probe(initial_delay_secs=5, exec=exec_action(['kubectl', 'get', "svc", "aperture-controller", "-n", APERTURE_CONTROLLER_NS])) # Treat as success if service is available
        local_resource(
            name="controller-service",
            labels="ApertureController",
            resource_deps=["controller"],
            serve_cmd=['./scripts/check-controller-service.sh'],
            readiness_probe=readiness_probe,
        )

    if config.tilt_subcommand == "down":
        if "agent" in resources:
            local('if (kubectl get agent agent -n {0}); ret=$?; [ $ret -eq 0 ]; then kubectl delete agent agent -n {0};  fi'.format(APERTURE_AGENT_NS))
        if "controller" in resources:
            local('if (kubectl get controller controller -n {0}); ret=$?; [ $ret -eq 0 ]; then kubectl delete controller controller -n {0};  fi'.format(APERTURE_CONTROLLER_NS))
        if "scenario-policies" in resources:
            resource_info = inv_dep_tree["scenario-policies"]
            for aperture_policy in resource_info.get("aperture_policies", []):
                policy_name = aperture_policy.get("policy_name")
                local('if (kubectl get policy {0} -n {1}); ret=$?; [ $ret -eq 0 ]; then kubectl delete policy {0} -n {1};  fi'.format(policy_name, APERTURE_CONTROLLER_NS))
        for resource_name in resources:
            resource = inv_dep_tree[resource_name]
            pvc_selectors = resource.get("pvc_selectors", [])
            for selector in pvc_selectors:
                schedule_pvc_deletion(resource["namespace"], selector)

    if SKIP_LOCAL_CMDS:
        fail("SKIP_LOCAL_CMDS mode enabled")


def process_dockerhub_images(dockerhub_images):
    for namespace, workloads in DEP_TREE.items():
        for workload, sections in workloads.items():
            if "images" not in sections:
                continue

            images_final = []
            for image in sections["images"]:
                image_ref = image["ref"]
                if image_ref not in dockerhub_images:
                    images_final += [image]

                images_final += [{"ref": image_ref, "tag": "latest"}]
            sections["images"] = images_final


def process_local_settings(local_settings):
    print("Applying local settings from ./tilt_local.json")
    override_default_registry = local_settings.get("default_registry", None)
    if override_default_registry:
        print("Overriding default registry to: "+ override_default_registry)
        default_registry(override_default_registry)

    override_allowed_k8s_contexts = local_settings.get("allowed_k8s_contexts", [])
    if override_allowed_k8s_contexts:
        print("Overriding allowed_k8s_contexts with: " + ",".join(override_allowed_k8s_contexts))
        allow_k8s_contexts(override_allowed_k8s_contexts)


############
### MAIN ###
############

# Parse config: https://docs.tilt.dev/tiltfile_config.html
config.define_string_list("to-run", args=True)
config.define_bool("list-resources", usage="Only list available resources and exit")
config.define_bool("manual", usage="Set trigger mode to manual")
config.define_bool("only", usage="Only deploy explicitly resources, without handling dependencies")
config.define_bool("verbose", usage="Enable verbose logging")
config.define_bool("trace", usage="Enable trace logging")
config.define_string("scenario", usage="Demo scenario to execute")
config.define_string_list("dockerhub-image", usage="A list of images to pull directly from docker.io/fluxninja")
config.define_bool("race",usage="Enable race detector for agent and controller")
config.define_bool("skip-plugins", usage="Skip plugin building")
config.define_bool("enable-cloud-plugin",usage="Enable ARC API key authentication, which will send the data to the ARC cloud")
cfg = config.parse()
# Enable all resources, as calling `config.parse()` by default disables all
config.set_enabled_resources([])

local_settings = read_json("tilt_local.json", None)
if local_settings:
    process_local_settings(local_settings)

if cfg.get("list-resources", False):
    print_available_resources(DEP_TREE)
    exit()


if cfg.get("manual", False):
    trigger_mode(TRIGGER_MODE_MANUAL)

get_race = cfg.get("race",False)
enable_cloud_plugin = cfg.get("enable-cloud-plugin",False)

run_only_listed = cfg.get("only", False)

# Optionally, process a list of images that should be pulled from dockerhub instead of being
# built locally. This is mostly meant for aperture-agent, aperture-controller and aperture-operator.
dockerhub_images = cfg.get("dockerhub-image", [])
if dockerhub_images:
    process_dockerhub_images(dockerhub_images)

to_run = cfg.get("to-run", ["agent"])

scenario = cfg.get("scenario", DEFAULT_TEST_SCENARIO if not run_only_listed else None)
if scenario:
    process_aperture_scenario(scenario)
    to_run += ["scenario-app", "scenario-wavepool"]
    # Not all scenarios have policies, so only load policies if they were generated based on the scenario.
    if "scenario-policies" in DEP_TREE[APERTURE_DEMOAPP_NS]:
        to_run += ["scenario-policies", "aperture-grafana"]

    for resource in DEP_TREE[APERTURE_CONTROLLER_NS]:
        if "scenario-dashboard" in resource:
            to_run += [resource]

cfg_trace = cfg.get("trace", os.getenv("NINJA_TILT_TRACE", "false"))
TRACE = cfg_trace == True

cfg_verbose = cfg.get("verbose", os.getenv("NINJA_TILT_VERBOSE", cfg_trace))
VERBOSE = cfg_verbose == True

if cfg.get("skip-plugins", os.getenv("SKIP_PLUGINS", "false").lower() == "true"):
    DEP_TREE[APERTURE_CONTROLLER_NS]["controller"]["images"][0]["target"] = "controller-base"
    DEP_TREE[APERTURE_AGENT_NS]["agent"]["images"][0]["target"] = "agent-base"

inverted_dep_tree = invert_dep_tree(DEP_TREE)
resources = get_resource_list_to_deploy(to_run, run_only_listed, DEP_TREE, inverted_dep_tree)

print("Will deploy resources:", ", ".join(resources))

# Watch charts and crd definitions
watch_file("Tiltfile")
watch_file("./scenarios/")
watch_file("./tanka/apps/")
watch_file("./tanka/lib/")
watch_file("./tanka/charts/")
watch_file(GIT_ROOT + "/operator/config/crd")

run_wavepool_graphql_generator = '''
kubectl -n demoapp scale --replicas=1 deployment wavepool-generator
'''.format(TANKA_ROOT)
cmd_button('wavepool-start',
    resource='wavepool-generator',
    icon_name='content_paste_go',
    text='Start Wavepool Generator',
    argv=['sh', '-c', run_wavepool_graphql_generator],
)

delete_wavepool_graphql_generator = '''
kubectl -n demoapp scale --replicas=0 deployment wavepool-generator
'''.format(APERTURE_DEMOAPP_NS)
cmd_button('wavepool-stop',
    resource='wavepool-generator',
    icon_name='content_paste_off',
    text='Stop Wavepool Generator',
    argv=['sh', '-c', delete_wavepool_graphql_generator],
)

aperture_go_pod_exec_script = '''
# get k8s pod name from tilt resource name
POD_NAME="$(tilt get kubernetesdiscovery "aperture-go-example" -ojsonpath='{.status.pods[0].name}')"
kubectl exec -n aperture-go-example "$POD_NAME" -- wget http://0.0.0.0:8080/super -O -
'''
cmd_button('aperture-go-trigger',
        argv=['sh', '-c', aperture_go_pod_exec_script],
        resource='aperture-go-example',
        icon_name='ads_click',
        text='Trigger library example',
)

aperture_js_pod_exec_script = '''
# get k8s pod name from tilt resource name
POD_NAME="$(tilt get kubernetesdiscovery "aperture-js-example" -ojsonpath='{.status.pods[0].name}')"
kubectl exec -n aperture-js-example "$POD_NAME" -- wget http://0.0.0.0:8080/super -O -
'''
cmd_button('aperture-js-trigger',
        argv=['sh', '-c', aperture_js_pod_exec_script],
        resource='aperture-js-example',
        icon_name='ads_click',
        text='Trigger library example',
)

aperture_java_pod_exec_script = '''
# get k8s pod name from tilt resource name
POD_NAME="$(tilt get kubernetesdiscovery "aperture-java-example" -ojsonpath='{.status.pods[0].name}')"
kubectl exec -n aperture-java-example "$POD_NAME" -- wget http://0.0.0.0:8080/super -O -
'''
cmd_button('aperture-java-trigger',
        argv=['sh', '-c', aperture_java_pod_exec_script],
        resource='aperture-java-example',
        icon_name='ads_click',
        text='Trigger library example',
)
declare_resources(resources, DEP_TREE, inverted_dep_tree,get_race, enable_cloud_plugin)
