---
title: Average Latency Feedback
sidebar_position: 1
keywords:
  - policies
  - service protection
  - concurrency limiting
---

```mdx-code-block
import {apertureVersion} from '../../apertureVersion.js';
import CodeBlock from '@theme/CodeBlock';
import Tabs from '@theme/Tabs';
import {BashTab, TabContent} from '../blueprintsComponents.js';
import Zoom from 'react-medium-image-zoom';
```

## Overview

Degradation in response latency can serve as a reliable signal for identifying
service overload. This policy is designed to detect overload situations based on
latency deterioration. During overload, the request rate is throttled so that
latency gets restored back to an acceptable range.

## Configuration

This policy is based on the
[Load Scheduling with Average Latency Feedback](/reference/blueprints/load-scheduling/average-latency.md)
blueprint. The latency of requests processed by the
**`cart-service.prod.svc.cluster.local`** service is monitored. By analyzing the
variance between current latency and a historical latency baseline, the policy
facilitates the identification of potential service overload. A deviation of
**`1.1`** from the baseline is considered as a signal of service overload.

In addition, workload prioritization is specified in the load scheduler based on
user types accessing the service. User types are identified based on the value
of a header label http.request.header.user_type. Requests matching label value
guest are assigned a priority of 50, whereas those matching subscriber are given
a priority of 200.

To mitigate service overload, the requests to the
**`cart-service.prod.svc.cluster.local`** service are passed through a load
scheduler. The load scheduler reduces the request rate in overload scenarios,
temporarily placing excess requests in a queue.

As service latency improves, indicating a return to normal operational state,
the request rate is incrementally increased until it matches the incoming
request rate. This responsive mechanism helps ensure that service performance is
optimized while mitigating the risk of service disruptions due to overload.

The below `values.yaml` file can be generated by following the steps in the
[Installation](#installation) section.

```mdx-code-block

<Tabs>
<TabItem value="aperturectl values.yaml">
```

```yaml
{@include: ./assets/average-latency-feedback/values.yaml}
```

```mdx-code-block
</TabItem>
</Tabs>
```

<details><summary>Generated Policy</summary>
<p>

```yaml
{@include: ./assets/average-latency-feedback/policy.yaml}
```

</p>
</details>

:::info

[Circuit Diagram](./assets/average-latency-feedback/graph.mmd.svg) for this
policy.

:::

## Installation

Generate a values file specific to the policy. This can be achieved using the
command provided below.

```mdx-code-block
<CodeBlock language="bash">aperturectl blueprints values --name=load-scheduling/average-latency --version={apertureVersion} --output-file=values.yaml</CodeBlock>
```

Apply the policy using the `aperturectl` CLI or `kubectl`.

```mdx-code-block
<Tabs>
  <TabItem value="aperturectl (Aperture Cloud)" label="aperturectl (Aperture Cloud)">
    <TabContent valuesFile="values" tabValue="aperturectl (Aperture Cloud)" />
  </TabItem>
  <TabItem value="aperturectl (self-hosted controller)" label="aperturectl (self-hosted controller)">
```

Pass the `--kube` flag with `aperturectl` to directly apply the generated policy
on a Kubernetes cluster in the namespace where the Aperture Controller is
installed.

```mdx-code-block
  <TabContent valuesFile="values" tabValue="aperturectl (self-hosted controller)" policyName="workload-prioritization" />
</TabItem>
<TabItem value="kubectl (self-hosted controller)" label="kubectl (self-hosted controller)">
```

Apply the generated policy YAML (Kubernetes Custom Resource) with `kubectl`.

```mdx-code-block
  <TabContent valuesFile="values" tabValue="kubectl (self-hosted controller)" policyName="workload-prioritization" />
</TabItem>

</Tabs>
```

## Policy in Action

To see the policy in action, the traffic is generated such that it starts within
the service's capacity and then goes beyond the capacity after some time. Such a
traffic pattern is repeated periodically. The below dashboard demonstrates that
when latency spikes due to high traffic at
`cart-service.prod.svc.cluster.local`, the Controller throttles the rate of
requests admitted into the service. This approach helps protect the service from
becoming unresponsive and maintains the current latency within the tolerance
limit (`1.1`) of historical latency.

![Basic Service Protection](./assets/average-latency-feedback/dashboard.png)

### Dry Run Mode

You can run this policy in the `Dry Run` mode by setting the
`policy.load_scheduling_core.dry_run` parameter to `true`. In the `Dry Run`
mode, the policy does not throttle the request rate while still evaluating the
decisions it would take in each cycle. This is useful for evaluating the policy
without impacting the service.

:::note

The `Dry Run` mode can also be toggled dynamically at runtime, without reloading
the policy.

:::

### Demo Video

The below demo video shows the basic service protection and workload
prioritization policy in action within Aperture Playground.

[![Demo Video](https://img.youtube.com/vi/m070bAvrDHM/0.jpg)](https://www.youtube.com/watch?v=m070bAvrDHM)
