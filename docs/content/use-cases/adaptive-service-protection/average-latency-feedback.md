---
title: Average Latency Feedback
keywords:
  - policies
  - concurrency
  - service-protection
sidebar_position: 2
---

```mdx-code-block
import {apertureVersion} from '../../apertureVersion.js';
import CodeBlock from '@theme/CodeBlock';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Zoom from 'react-medium-image-zoom';
```

## Overview

The response times of a service start to deteriorate when the service's
underlying concurrency limit is surpassed. Consequently, a degradation in
response latency can serve as a reliable signal for identifying service
overload. This policy is designed to detect overload situations based on latency
deterioration. During overload, the request rate is throttled so that latency
gets restored back to an acceptable range.

## Configuration

This policy is based on the
[Load Scheduling with Average Latency Feedback](/reference/blueprints/load-scheduling/average-latency.md)
blueprint. The latency of requests processed by the
**`cart-service.prod.svc.cluster.local`** service is monitored. By analyzing the
variance between current latency and a historical latency baseline, the policy
facilitates the identification of potential service overload. A deviation of
**`1.1`** from the baseline is considered as a signal of service overload.

To mitigate service overload, the requests to the
**`cart-service.prod.svc.cluster.local`** service are passed through a load
scheduler. The load scheduler reduces the request rate in overload scenarios,
temporarily placing excess requests in a queue.

As service latency improves, indicating a return to normal operational state,
the request rate is incrementally increased until it matches the incoming
request rate. This responsive mechanism helps ensure that service performance is
optimized while mitigating the risk of service disruptions due to overload.

The below `values.yaml` file can be generated by following the steps in the
[Installation](#installation) section.

```mdx-code-block

<Tabs>
<TabItem value="aperturectl values.yaml">
```

```yaml
{@include: ./assets/average-latency-feedback/values.yaml}
```

```mdx-code-block
</TabItem>
</Tabs>
```

<details><summary>Generated Policy</summary>
<p>

```yaml
{@include: ./assets/average-latency-feedback/policy.yaml}
```

</p>
</details>

:::info

[Circuit Diagram](./assets/average-latency-feedback/graph.mmd.svg) for this
policy.

:::

## Installation

Generate a values file specific to the policy. This can be achieved using the
command provided below.

```mdx-code-block
<CodeBlock language="bash">aperturectl blueprints values --name=load-scheduling/average-latency --version={apertureVersion} --output-file=values.yaml</CodeBlock>
```

Adjust the values to match the application requirements. Use the following
command to generate the policy.

```mdx-code-block
<CodeBlock language="bash">aperturectl blueprints generate --values-file=values.yaml --output-dir=policy-gen</CodeBlock>
```

Apply the policy using the `aperturectl` CLI or `kubectl`.

```mdx-code-block
<Tabs>
<TabItem value="aperturectl" label="aperturectl">
```

Pass the `--kube` flag with `aperturectl` to directly apply the generated policy
on a Kubernetes cluster in the namespace where the Aperture Controller is
installed.

```mdx-code-block
<CodeBlock language="bash">aperturectl apply policy --file=policy-gen/policies/basic-service-protection.yaml --kube </CodeBlock>
```

```mdx-code-block
</TabItem>
<TabItem value="kubectl" label="kubectl">
```

Apply the policy YAML generated (Kubernetes Custom Resource) using the above
example with `kubectl`.

```bash
kubectl apply -f policy-gen/configuration/basic-service-protection-cr.yaml -n aperture-controller
```

```mdx-code-block
</TabItem>
</Tabs>
```

## Policy in Action

To see the policy in action, the traffic is generated such that it starts within
the service's capacity and then goes beyond the capacity after some time. Such a
traffic pattern is repeated periodically. The below dashboard demonstrates that
when latency spikes due to high traffic at
`cart-service.prod.svc.cluster.local`, the Controller throttles the rate of
requests admitted into the service. This approach helps protect the service from
becoming unresponsive and maintains the current latency within the tolerance
limit (`1.1`) of historical latency.

![Basic Service Protection](./assets/average-latency-feedback/dashboard.png)

### Dry Run Mode

You can run this policy in the `Dry Run` mode by setting the
`policy.service_protection_core.dry_run` parameter to `true`. In the `Dry Run`
mode, the policy does not throttle the request rate while still evaluating the
decisions it would take in each cycle. This is useful for evaluating the policy
without impacting the service.

:::note

The `Dry Run` mode can also be toggled dynamically at runtime, without reloading
the policy.

:::

### Demo Video

The below demo video shows the basic service protection and workload
prioritization policy in action within Aperture Playground.

[![Demo Video](https://img.youtube.com/vi/m070bAvrDHM/0.jpg)](https://www.youtube.com/watch?v=m070bAvrDHM)
